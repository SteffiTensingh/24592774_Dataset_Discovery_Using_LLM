{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import Trainer, TrainingArguments, T5ForConditionalGeneration, T5Tokenizer\n",
    "from datasets import load_metric\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Metadata = pd.read_csv('C:\\\\24592774_Dataset_Discovery_Using_LLM\\\\MetaData_Creation\\\\MetaData_Notebooks\\\\Prepared_MetaData_DataSet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>summary</th>\n",
       "      <th>tags</th>\n",
       "      <th>dataset_url</th>\n",
       "      <th>available_formats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['2023w7', 'global', 'electric', 'vehicle', 'm...</td>\n",
       "      <td>['global', 'electric', 'vehicle', 'market', 's...</td>\n",
       "      <td>['editorsimple', 'original', 'visualization', ...</td>\n",
       "      <td>['makeover monday', 'cars', 'vehicles', 'elect...</td>\n",
       "      <td>https://data.world/makeovermonday/2023w7</td>\n",
       "      <td>['xlsx']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['connected', 'electric', 'autonomous', 'vehic...</td>\n",
       "      <td>['connected', 'electric', 'autonomous', 'vehic...</td>\n",
       "      <td>['rolling', 'snapshot', 'data', 'collected', '...</td>\n",
       "      <td>['autonomous']</td>\n",
       "      <td>https://data.world/smartcolumbusos/650b7e59-af...</td>\n",
       "      <td>['csv']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['electric', 'vehicle', 'charging', 'stations']</td>\n",
       "      <td>['dataset', 'contains', 'session', 'details', ...</td>\n",
       "      <td>['dataset', 'contains', 'session', 'details', ...</td>\n",
       "      <td>['electric vehicle', 'environment', 'energy']</td>\n",
       "      <td>https://data.world/townofcary/electric-vehicle...</td>\n",
       "      <td>['dbf', 'csv', 'shx', 'shp', 'json', 'prj']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['nyserda', 'electric', 'vehicle', 'drive', 'c...</td>\n",
       "      <td>['new', 'york', 'state', '’', 'charge', 'ny', ...</td>\n",
       "      <td>['original', 'title', 'nyserda', 'electric', '...</td>\n",
       "      <td>['ev', 'electric vehicle', 'bev', 'phev', 'ghg...</td>\n",
       "      <td>https://data.world/data-ny-gov/thd2-fu8y</td>\n",
       "      <td>['csv']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['impact', 'uncoordinated', 'plugin', 'electri...</td>\n",
       "      <td>['impact', 'uncoordinated', 'plugin', 'electri...</td>\n",
       "      <td>['original', 'title', 'impact', 'uncoordinated...</td>\n",
       "      <td>['battery', 'consumption', 'data', 'energy', '...</td>\n",
       "      <td>https://data.world/us-doe-gov/8ae7e117-313b-40...</td>\n",
       "      <td>['xlsx']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  ['2023w7', 'global', 'electric', 'vehicle', 'm...   \n",
       "1  ['connected', 'electric', 'autonomous', 'vehic...   \n",
       "2    ['electric', 'vehicle', 'charging', 'stations']   \n",
       "3  ['nyserda', 'electric', 'vehicle', 'drive', 'c...   \n",
       "4  ['impact', 'uncoordinated', 'plugin', 'electri...   \n",
       "\n",
       "                                         description  \\\n",
       "0  ['global', 'electric', 'vehicle', 'market', 's...   \n",
       "1  ['connected', 'electric', 'autonomous', 'vehic...   \n",
       "2  ['dataset', 'contains', 'session', 'details', ...   \n",
       "3  ['new', 'york', 'state', '’', 'charge', 'ny', ...   \n",
       "4  ['impact', 'uncoordinated', 'plugin', 'electri...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  ['editorsimple', 'original', 'visualization', ...   \n",
       "1  ['rolling', 'snapshot', 'data', 'collected', '...   \n",
       "2  ['dataset', 'contains', 'session', 'details', ...   \n",
       "3  ['original', 'title', 'nyserda', 'electric', '...   \n",
       "4  ['original', 'title', 'impact', 'uncoordinated...   \n",
       "\n",
       "                                                tags  \\\n",
       "0  ['makeover monday', 'cars', 'vehicles', 'elect...   \n",
       "1                                     ['autonomous']   \n",
       "2      ['electric vehicle', 'environment', 'energy']   \n",
       "3  ['ev', 'electric vehicle', 'bev', 'phev', 'ghg...   \n",
       "4  ['battery', 'consumption', 'data', 'energy', '...   \n",
       "\n",
       "                                         dataset_url  \\\n",
       "0           https://data.world/makeovermonday/2023w7   \n",
       "1  https://data.world/smartcolumbusos/650b7e59-af...   \n",
       "2  https://data.world/townofcary/electric-vehicle...   \n",
       "3           https://data.world/data-ny-gov/thd2-fu8y   \n",
       "4  https://data.world/us-doe-gov/8ae7e117-313b-40...   \n",
       "\n",
       "                             available_formats  \n",
       "0                                     ['xlsx']  \n",
       "1                                      ['csv']  \n",
       "2  ['dbf', 'csv', 'shx', 'shp', 'json', 'prj']  \n",
       "3                                      ['csv']  \n",
       "4                                     ['xlsx']  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Metadata.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Values:\n",
      "title                0\n",
      "description          0\n",
      "summary              0\n",
      "tags                 0\n",
      "dataset_url          0\n",
      "available_formats    0\n",
      "dtype: int64\n",
      "\n",
      "NA Values:\n",
      "title                0\n",
      "description          0\n",
      "summary              0\n",
      "tags                 0\n",
      "dataset_url          0\n",
      "available_formats    0\n",
      "dtype: int64\n",
      "\n",
      "NaN Values:\n",
      "title                0\n",
      "description          0\n",
      "summary              0\n",
      "tags                 0\n",
      "dataset_url          0\n",
      "available_formats    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Check for null values in each column\n",
    "null_values = Metadata.isnull().sum()\n",
    "print(\"Null Values:\")\n",
    "print(null_values)\n",
    "\n",
    "# Check for NA values in each column\n",
    "na_values = Metadata.isna().sum()\n",
    "print(\"\\nNA Values:\")\n",
    "print(na_values)\n",
    "\n",
    "# Check for NaN values in each column\n",
    "nan_values = Metadata.isna().sum()\n",
    "print(\"\\nNaN Values:\")\n",
    "print(nan_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Temp\\ipykernel_30308\\4220371594.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_ids = torch.tensor(train_ids)\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Temp\\ipykernel_30308\\4220371594.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_ids = torch.tensor(val_ids)\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Temp\\ipykernel_30308\\4220371594.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_masks = torch.tensor(train_masks)\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Temp\\ipykernel_30308\\4220371594.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_masks = torch.tensor(val_masks)\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 0.2304468871850986, Train Accuracy: 0.8059701492537313\n",
      "Validation Loss: 0.00011329429253237322, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 2/10\n",
      "Train Loss: 0.00014587936893804, Train Accuracy: 1.0\n",
      "Validation Loss: 8.16580268292455e-06, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 3/10\n",
      "Train Loss: 1.3201191723055673e-05, Train Accuracy: 1.0\n",
      "Validation Loss: 4.436810968400096e-06, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 4/10\n",
      "Train Loss: 6.776504778827075e-06, Train Accuracy: 1.0\n",
      "Validation Loss: 3.699206217788742e-06, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 5/10\n",
      "Train Loss: 5.848165801580762e-06, Train Accuracy: 1.0\n",
      "Validation Loss: 3.397458840481704e-06, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 6/10\n",
      "Train Loss: 4.063034793944098e-06, Train Accuracy: 1.0\n",
      "Validation Loss: 3.1366895427709096e-06, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 7/10\n",
      "Train Loss: 4.617850800059387e-06, Train Accuracy: 1.0\n",
      "Validation Loss: 2.890820951506612e-06, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 8/10\n",
      "Train Loss: 3.575273922251654e-06, Train Accuracy: 1.0\n",
      "Validation Loss: 2.644952360242314e-06, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 9/10\n",
      "Train Loss: 3.3820585485955236e-06, Train Accuracy: 1.0\n",
      "Validation Loss: 2.4810401555441786e-06, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 10/10\n",
      "Train Loss: 3.947788991354173e-06, Train Accuracy: 1.0\n",
      "Validation Loss: 2.2463475488621043e-06, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AlbertTokenizer, AlbertModel, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "MetaData_DS = Metadata.copy()\n",
    "\n",
    "# ALBERT tokenizer\n",
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "\n",
    "max_length = 128  \n",
    "\n",
    "# Tokenize dataset descriptions\n",
    "tokenized_descriptions = tokenizer(list(MetaData_DS['description']), truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
    "\n",
    "labels = [1 if url else 0 for url in MetaData_DS['dataset_url']]\n",
    "\n",
    "# Extractions\n",
    "input_ids = tokenized_descriptions['input_ids']\n",
    "attention_mask = tokenized_descriptions['attention_mask']\n",
    "\n",
    "# training and validation sets splitting\n",
    "train_ids, val_ids, train_masks, val_masks, train_labels, val_labels = train_test_split(input_ids, attention_mask, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "train_ids = torch.tensor(train_ids)\n",
    "val_ids = torch.tensor(val_ids)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "#  TensorDataset for training and validation sets creation\n",
    "train_dataset = TensorDataset(train_ids, train_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_ids, val_masks, val_labels)\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the ALBERT-based model \n",
    "class AlbertClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(AlbertClassifier, self).__init__()\n",
    "        self.albert = AlbertModel.from_pretrained('albert-base-v2')\n",
    "        self.dropout = nn.Dropout(0.3)  # Dropout rate\n",
    "        self.linear = nn.Linear(self.albert.config.hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.albert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = outputs.last_hidden_state[:, 0]  # Take [CLS] token representation\n",
    "        hidden_state = self.dropout(hidden_state)\n",
    "        logits = self.linear(hidden_state)\n",
    "        return logits\n",
    "\n",
    "#  ALBERT model\n",
    "num_classes = 2  \n",
    "model = AlbertClassifier(num_classes)\n",
    "\n",
    "# Define the optimizer with weight decay and gradient clipping\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=1, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop with early stopping\n",
    "epochs = 10\n",
    "patience = 2\n",
    "best_loss = float('inf')\n",
    "trigger_times = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for batch in train_loader:\n",
    "        inputs, attention_masks, labels = batch\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs, attention_masks)\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy for this batch\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = total_correct / total_samples\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, attention_masks, labels = batch\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs, attention_masks)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Compute predictions\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "            # Update total samples and total correct predictions\n",
    "            total_samples += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Save predictions and labels for metric calculation\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='binary')\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    print(f\"Train Loss: {avg_train_loss}, Train Accuracy: {train_accuracy}\")\n",
    "    print(f\"Validation Loss: {avg_val_loss}, Validation Accuracy: {accuracy}\")\n",
    "    print(f\"Validation Precision: {precision}, Validation Recall: {recall}, Validation F1 Score: {f1}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        trigger_times = 0\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print('Early stopping!')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MetaData_DS['text'] = MetaData_DS['title']+MetaData_DS['description']+MetaData_DS['summary']+MetaData_DS['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Temp\\ipykernel_30308\\3591330173.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_ids = torch.tensor(train_ids)\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Temp\\ipykernel_30308\\3591330173.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_ids = torch.tensor(val_ids)\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Temp\\ipykernel_30308\\3591330173.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_masks = torch.tensor(train_masks)\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Temp\\ipykernel_30308\\3591330173.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_masks = torch.tensor(val_masks)\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 0.32286162620875986, Train Accuracy: 0.7761194029850746\n",
      "Validation Loss: 0.00014088406169321388, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 2/10\n",
      "Train Loss: 7.695886652072658e-05, Train Accuracy: 1.0\n",
      "Validation Loss: 5.006777200833312e-06, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 3/10\n",
      "Train Loss: 4.995345625502523e-06, Train Accuracy: 1.0\n",
      "Validation Loss: 2.082434832573199e-06, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 4/10\n",
      "Train Loss: 3.4694752685027196e-06, Train Accuracy: 1.0\n",
      "Validation Loss: 1.4379608614945028e-06, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 5/10\n",
      "Train Loss: 1.8805239733410418e-06, Train Accuracy: 1.0\n",
      "Validation Loss: 1.1362127452230197e-06, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 6/10\n",
      "Train Loss: 1.9699295535247076e-06, Train Accuracy: 1.0\n",
      "Validation Loss: 1.054256642873952e-06, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 7/10\n",
      "Train Loss: 1.9237370906921568e-06, Train Accuracy: 1.0\n",
      "Validation Loss: 1.005827868993947e-06, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 8/10\n",
      "Train Loss: 2.1854991018699364e-06, Train Accuracy: 1.0\n",
      "Validation Loss: 9.05245201465732e-07, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 9/10\n",
      "Train Loss: 1.4404438616111292e-06, Train Accuracy: 1.0\n",
      "Validation Loss: 8.903439834284654e-07, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 10/10\n",
      "Train Loss: 1.4384562177838233e-06, Train Accuracy: 1.0\n",
      "Validation Loss: 8.717175887795747e-07, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AlbertTokenizer, AlbertModel, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "# Initialize ALBERT tokenizer\n",
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "\n",
    "max_length = 128  \n",
    "# Tokenize dataset descriptions\n",
    "tokenized_descriptions = tokenizer(list(MetaData_DS['text']), truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
    "\n",
    "# Convert labels to numerical format\n",
    "labels = [1 if url else 0 for url in MetaData_DS['dataset_url']]\n",
    "\n",
    "# Extract input_ids and attention_mask\n",
    "input_ids = tokenized_descriptions['input_ids']\n",
    "attention_mask = tokenized_descriptions['attention_mask']\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_ids, val_ids, train_masks, val_masks, train_labels, val_labels = train_test_split(input_ids, attention_mask, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert tokenized texts and labels to PyTorch tensors\n",
    "train_ids = torch.tensor(train_ids)\n",
    "val_ids = torch.tensor(val_ids)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "# Create TensorDataset for training and validation sets\n",
    "train_dataset = TensorDataset(train_ids, train_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_ids, val_masks, val_labels)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Create DataLoader for training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the ALBERT-based model with increased dropout\n",
    "class AlbertClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(AlbertClassifier, self).__init__()\n",
    "        self.albert = AlbertModel.from_pretrained('albert-base-v2')\n",
    "        self.dropout = nn.Dropout(0.3)  # Dropout rate\n",
    "        self.linear = nn.Linear(self.albert.config.hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.albert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = outputs.last_hidden_state[:, 0]  # Take [CLS] token representation\n",
    "        hidden_state = self.dropout(hidden_state)\n",
    "        logits = self.linear(hidden_state)\n",
    "        return logits\n",
    "\n",
    "# Instantiate the ALBERT model\n",
    "num_classes = 2  \n",
    "model = AlbertClassifier(num_classes)\n",
    "\n",
    "# Define the optimizer with weight decay and gradient clipping\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=1, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop with early stopping\n",
    "epochs = 10\n",
    "patience = 2\n",
    "best_loss = float('inf')\n",
    "trigger_times = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for batch in train_loader:\n",
    "        inputs, attention_masks, labels = batch\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs, attention_masks)\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy for this batch\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = total_correct / total_samples\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, attention_masks, labels = batch\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs, attention_masks)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Compute predictions\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "            # Update total samples and total correct predictions\n",
    "            total_samples += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Save predictions and labels for metric calculation\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='binary')\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    print(f\"Train Loss: {avg_train_loss}, Train Accuracy: {train_accuracy}\")\n",
    "    print(f\"Validation Loss: {avg_val_loss}, Validation Accuracy: {accuracy}\")\n",
    "    print(f\"Validation Precision: {precision}, Validation Recall: {recall}, Validation F1 Score: {f1}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        trigger_times = 0\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print('Early stopping!')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Temp\\ipykernel_30308\\134244627.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_ids = torch.tensor(train_ids)\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Temp\\ipykernel_30308\\134244627.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_ids = torch.tensor(val_ids)\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Temp\\ipykernel_30308\\134244627.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_masks = torch.tensor(train_masks)\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Temp\\ipykernel_30308\\134244627.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_masks = torch.tensor(val_masks)\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Train Loss: 0.41791629791259766, Train Accuracy: 0.8208955223880597\n",
      "Validation Loss: 0.07898492366075516, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 2/3\n",
      "Train Loss: 0.05128904189914465, Train Accuracy: 1.0\n",
      "Validation Loss: 0.011473944410681725, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 3/3\n",
      "Train Loss: 0.0091940822545439, Train Accuracy: 1.0\n",
      "Validation Loss: 0.0038527388824149966, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AlbertTokenizer, AlbertModel, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Initialize ALBERT tokenizer\n",
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "\n",
    "max_length = 128  \n",
    "\n",
    "# Tokenize dataset descriptions\n",
    "tokenized_descriptions = tokenizer(list(MetaData_DS['text']), truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
    "\n",
    "# Convert labels to numerical format\n",
    "labels = [1 if url else 0 for url in MetaData_DS['dataset_url']]\n",
    "\n",
    "# Extract input_ids and attention_mask\n",
    "input_ids = tokenized_descriptions['input_ids']\n",
    "attention_mask = tokenized_descriptions['attention_mask']\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_ids, val_ids, train_masks, val_masks, train_labels, val_labels = train_test_split(input_ids, attention_mask, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert tokenized texts and labels to PyTorch tensors\n",
    "train_ids = torch.tensor(train_ids)\n",
    "val_ids = torch.tensor(val_ids)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "# Create TensorDataset for training and validation sets\n",
    "train_dataset = TensorDataset(train_ids, train_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_ids, val_masks, val_labels)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Create DataLoader for training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the ALBERT-based model with increased dropout\n",
    "class AlbertClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(AlbertClassifier, self).__init__()\n",
    "        self.albert = AlbertModel.from_pretrained('albert-base-v2')\n",
    "        self.dropout = nn.Dropout(0.5)  \n",
    "        self.linear = nn.Linear(self.albert.config.hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.albert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output  # Use pooler_output instead of last_hidden_state\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.linear(pooled_output)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Instantiate the ALBERT model\n",
    "num_classes = 2  \n",
    "model = AlbertClassifier(num_classes)\n",
    "\n",
    "# Define the optimizer with weight decay and gradient clipping\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=1, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop with early stopping\n",
    "epochs = 3\n",
    "patience = 2\n",
    "best_loss = float('inf')\n",
    "trigger_times = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for batch in train_loader:\n",
    "        inputs, attention_masks, labels = batch\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs, attention_masks)\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy for this batch\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = total_correct / total_samples\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, attention_masks, labels = batch\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs, attention_masks)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Compute predictions\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "            # Update total samples and total correct predictions\n",
    "            total_samples += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Save predictions and labels for metric calculation\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='binary')\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    print(f\"Train Loss: {avg_train_loss}, Train Accuracy: {train_accuracy}\")\n",
    "    print(f\"Validation Loss: {avg_val_loss}, Validation Accuracy: {accuracy}\")\n",
    "    print(f\"Validation Precision: {precision}, Validation Recall: {recall}, Validation F1 Score: {f1}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        trigger_times = 0\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print('Early stopping!')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Temp\\ipykernel_30308\\4220371594.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_ids = torch.tensor(train_ids)\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Temp\\ipykernel_30308\\4220371594.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_ids = torch.tensor(val_ids)\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Temp\\ipykernel_30308\\4220371594.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_masks = torch.tensor(train_masks)\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Temp\\ipykernel_30308\\4220371594.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_masks = torch.tensor(val_masks)\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 0.33168293490889483, Train Accuracy: 0.7761194029850746\n",
      "Validation Loss: 0.00020222133025527, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 2/10\n",
      "Train Loss: 0.0002504504878743319, Train Accuracy: 1.0\n",
      "Validation Loss: 1.4580677088815719e-05, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 3/10\n",
      "Train Loss: 1.4306911361927633e-05, Train Accuracy: 1.0\n",
      "Validation Loss: 9.890595265460433e-06, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 4/10\n",
      "Train Loss: 1.2289830101508414e-05, Train Accuracy: 1.0\n",
      "Validation Loss: 7.040772743494017e-06, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 5/10\n",
      "Train Loss: 9.123415384237888e-06, Train Accuracy: 1.0\n",
      "Validation Loss: 5.092458422950585e-06, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 6/10\n",
      "Train Loss: 5.257100428934791e-06, Train Accuracy: 1.0\n",
      "Validation Loss: 3.840766680696106e-06, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 7/10\n",
      "Train Loss: 4.904441084363498e-06, Train Accuracy: 1.0\n",
      "Validation Loss: 3.017480480593804e-06, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 8/10\n",
      "Train Loss: 3.500768480080296e-06, Train Accuracy: 1.0\n",
      "Validation Loss: 2.481040041857341e-06, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 9/10\n",
      "Train Loss: 3.762027972697979e-06, Train Accuracy: 1.0\n",
      "Validation Loss: 2.0787097696484125e-06, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 10/10\n",
      "Train Loss: 4.007400457339827e-06, Train Accuracy: 1.0\n",
      "Validation Loss: 1.7918629282576148e-06, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AlbertTokenizer, AlbertModel, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "MetaData_DS = Metadata.copy()\n",
    "\n",
    "# Initialize ALBERT tokenizer\n",
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "\n",
    "max_length = 128  \n",
    "\n",
    "# Tokenize dataset descriptions\n",
    "tokenized_descriptions = tokenizer(list(MetaData_DS['description']), truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
    "\n",
    "# Convert labels to numerical format\n",
    "labels = [1 if url else 0 for url in MetaData_DS['dataset_url']]\n",
    "\n",
    "# Extract input_ids and attention_mask\n",
    "input_ids = tokenized_descriptions['input_ids']\n",
    "attention_mask = tokenized_descriptions['attention_mask']\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_ids, val_ids, train_masks, val_masks, train_labels, val_labels = train_test_split(input_ids, attention_mask, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert tokenized texts and labels to PyTorch tensors\n",
    "train_ids = torch.tensor(train_ids)\n",
    "val_ids = torch.tensor(val_ids)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "# Create TensorDataset for training and validation sets\n",
    "train_dataset = TensorDataset(train_ids, train_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_ids, val_masks, val_labels)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Create DataLoader for training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the ALBERT-based model with increased dropout\n",
    "class AlbertClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(AlbertClassifier, self).__init__()\n",
    "        self.albert = AlbertModel.from_pretrained('albert-base-v2')\n",
    "        self.dropout = nn.Dropout(0.3)  # Dropout rate\n",
    "        self.linear = nn.Linear(self.albert.config.hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.albert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = outputs.last_hidden_state[:, 0]  # Take [CLS] token representation\n",
    "        hidden_state = self.dropout(hidden_state)\n",
    "        logits = self.linear(hidden_state)\n",
    "        return logits\n",
    "\n",
    "# Instantiate the ALBERT model\n",
    "num_classes = 2 \n",
    "model = AlbertClassifier(num_classes)\n",
    "\n",
    "# Define the optimizer with weight decay and gradient clipping\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=1, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop with early stopping\n",
    "epochs = 10\n",
    "patience = 2\n",
    "best_loss = float('inf')\n",
    "trigger_times = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for batch in train_loader:\n",
    "        inputs, attention_masks, labels = batch\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs, attention_masks)\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy for this batch\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = total_correct / total_samples\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, attention_masks, labels = batch\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs, attention_masks)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Compute predictions\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "            # Update total samples and total correct predictions\n",
    "            total_samples += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Save predictions and labels for metric calculation\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='binary')\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    print(f\"Train Loss: {avg_train_loss}, Train Accuracy: {train_accuracy}\")\n",
    "    print(f\"Validation Loss: {avg_val_loss}, Validation Accuracy: {accuracy}\")\n",
    "    print(f\"Validation Precision: {precision}, Validation Recall: {recall}, Validation F1 Score: {f1}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        trigger_times = 0\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print('Early stopping!')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlbertTokenizer, AlbertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load ALBERT tokenizer and model\n",
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "model = AlbertModel.from_pretrained('albert-base-v2')\n",
    "model.eval()\n",
    "\n",
    "# Function to get ALBERT embedding\n",
    "def get_albert_embedding(text, model, tokenizer, max_length=128):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=max_length)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use the last hidden state of the [CLS] token as the embedding\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlbertTokenizer, AlbertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load ALBERT tokenizer and model\n",
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "model = AlbertModel.from_pretrained('albert-base-v2')\n",
    "model.eval()\n",
    "\n",
    "# Function to get ALBERT embedding\n",
    "def get_albert_embedding(text, model, tokenizer, max_length=128):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=max_length)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use the last hidden state of the [CLS] token as the embedding\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_descriptions = Metadata['description']\n",
    "\n",
    "# Generate embeddings for all dataset descriptions\n",
    "dataset_embeddings = []\n",
    "for description in dataset_descriptions:\n",
    "    emb = get_albert_embedding(description, model, tokenizer)\n",
    "    dataset_embeddings.append(emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ['nyserda', 'electric', 'vehicle', 'drive', 'clean', 'rebate', 'data', '2017']\n",
      "Dataset URL: https://data.world/data-ny-gov/thd2-fu8y\n",
      "Cosine Similarity: 0.7907650470733643\n",
      "\n",
      "Title: ['hawaii', 'public', 'electric', 'vehicle', 'charging', 'stations']\n",
      "Dataset URL: https://data.world/johnsnowlabs/hawaii-public-electric-vehicle-charging-stations\n",
      "Cosine Similarity: 0.7595440149307251\n",
      "\n",
      "Title: ['romanian', 'new', 'car', 'registration', '2023']\n",
      "Dataset URL: https://data.world/romanian-data/romanian-new-car-registration-in-2023\n",
      "Cosine Similarity: 0.7753380537033081\n",
      "\n",
      "Title: ['site', 'g03', 'gasconade', 'river', 'bathymetry', 'structure', 'a1411', '89']\n",
      "Dataset URL: https://data.world/us-doi-gov/12749821-f445-4f35-8dee-81eb7a56f07d\n",
      "Cosine Similarity: 0.8071860074996948\n",
      "\n",
      "Title: ['gpm', 'ground', 'validation', 'twodimensional', 'video', 'disdromete', 'v1']\n",
      "Dataset URL: https://data.world/us-nasa-gov/47e3817a-65aa-4af4-8d40-f0ed0f655bda\n",
      "Cosine Similarity: 0.7865433096885681\n",
      "\n",
      "Title: ['un', 'population', 'growth', 'rate', 'xl']\n",
      "Dataset URL: https://data.world/brianray/un-population-growth-rate-xl\n",
      "Cosine Similarity: 0.7657961845397949\n",
      "\n",
      "Title: ['okgoogleassistant']\n",
      "Dataset URL: https://data.world/okgoogleassistant/okgoogleassistant\n",
      "Cosine Similarity: 0.7697305679321289\n",
      "\n",
      "Title: ['test1234']\n",
      "Dataset URL: https://data.world/kgs1/test1234\n",
      "Cosine Similarity: 0.791000485420227\n",
      "\n",
      "Title: ['imdb', 'movies']\n",
      "Dataset URL: https://data.world/eng/imdb-movies\n",
      "Cosine Similarity: 0.7974239587783813\n",
      "\n",
      "Title: ['legalmap']\n",
      "Dataset URL: https://data.world/h0tftw/legalmap\n",
      "Cosine Similarity: 0.791000485420227\n",
      "\n",
      "Title: ['abortion', 'pills', 'price', 'dubai', '00971551624914', 'abortion', 'pills']\n",
      "Dataset URL: https://data.world/cytotecpillsdubai/abortion-pills-price-in-dubai-00971551624914-abortion-pills\n",
      "Cosine Similarity: 0.7588078379631042\n",
      "\n",
      "Title: ['buy', 'wireless', 'security', 'cameras', 'affordable', 'prices']\n",
      "Dataset URL: https://data.world/kongkei/buy-wireless-security-cameras-at-affordable-prices\n",
      "Cosine Similarity: 0.7670471668243408\n",
      "\n",
      "Title: ['iptv', 'subscription', 'service', 'go']\n",
      "Dataset URL: https://data.world/freemotion/which-iptv-subscription-service-should-you-go-for\n",
      "Cosine Similarity: 0.791000485420227\n",
      "\n",
      "Title: ['engineering', 'cyanobacteria', 'production', 'lightweig']\n",
      "Dataset URL: https://data.world/us-nasa-gov/8f4b3f7a-a44b-42c3-b11e-09f7e2c96edd\n",
      "Cosine Similarity: 0.7546228170394897\n",
      "\n",
      "Title: ['rational', 'engineering', 'carbon', 'nanotube', 'surfaces', 'phase']\n",
      "Dataset URL: https://data.world/us-nasa-gov/12ca296c-38fb-4b24-a6b6-a3b5ed1c1eb5\n",
      "Cosine Similarity: 0.7655909061431885\n",
      "\n",
      "Title: ['food', 'price', 'outlook']\n",
      "Dataset URL: https://data.world/agriculture/food-price-outlook\n",
      "Cosine Similarity: 0.779902458190918\n",
      "\n",
      "Title: ['food', 'environment', 'atlas']\n",
      "Dataset URL: https://data.world/agriculture/food-environment-atlas\n",
      "Cosine Similarity: 0.7575954794883728\n",
      "\n",
      "Title: ['recognized', 'sports']\n",
      "Dataset URL: https://data.world/sports/recognized-sports\n",
      "Cosine Similarity: 0.791000485420227\n",
      "\n",
      "Title: ['sports', 'illustrated', 'covers']\n",
      "Dataset URL: https://data.world/crowdflower/sports-illustrated-covers\n",
      "Cosine Similarity: 0.791000485420227\n",
      "\n",
      "Title: ['trends', 'youth', 'sports']\n",
      "Dataset URL: https://data.world/zendoll27/trends-in-youth-sports\n",
      "Cosine Similarity: 0.7625715732574463\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to predict dataset URLs based on search text\n",
    "def predict_dataset_url(search_text):\n",
    "    search_emb = get_albert_embedding(search_text, model, tokenizer)\n",
    "    similarities = [cosine_similarity([search_emb], [dataset_emb])[0][0] for dataset_emb in dataset_embeddings]\n",
    "    \n",
    "    # Find the maximum and minimum similarity values\n",
    "    max_similarity = max(similarities)\n",
    "    min_similarity = min(similarities)\n",
    "    \n",
    "    # Calculate the 90% range\n",
    "    range_90 = 0.50 * (max_similarity - min_similarity)\n",
    "    \n",
    "    # Find the threshold value\n",
    "    threshold = min_similarity + range_90\n",
    "    \n",
    "    # Find the indices of datasets with similarity above the threshold\n",
    "    similar_indices = [i for i, sim in enumerate(similarities) if sim > threshold]\n",
    "    \n",
    "    results = []\n",
    "    for idx in similar_indices:\n",
    "        dataset_info = {\n",
    "            \"title\": Metadata.iloc[idx]['title'],\n",
    "            \"dataset_url\": Metadata.iloc[idx]['dataset_url'],\n",
    "            \"cosine_similarity\": similarities[idx]\n",
    "        }\n",
    "        results.append(dataset_info)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "search_text = \"electric charging points\"\n",
    "results = predict_dataset_url(search_text)\n",
    "\n",
    "# Display results\n",
    "if not results:\n",
    "    print(\"No datasets found above the 90% threshold.\")\n",
    "else:\n",
    "    for result in results:\n",
    "        print(f\"Title: {result['title']}\")\n",
    "        print(f\"Dataset URL: {result['dataset_url']}\")\n",
    "        print(f\"Cosine Similarity: {result['cosine_similarity']}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
