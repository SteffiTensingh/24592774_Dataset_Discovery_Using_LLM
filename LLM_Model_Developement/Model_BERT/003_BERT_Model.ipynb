{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model and tokenizer for classification\n",
    "tokenizer_cls = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_cls = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "# Load pre-trained BERT model and tokenizer for similarity\n",
    "tokenizer_sim = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_sim = BertModel.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labeled_Metadata =  pd.read_csv('C:\\\\Users\\\\Steffi Grace\\\\24592774_LLM_ILabResearch\\\\Notebooks\\\\Datasets\\\\Labeled_MetaData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text data\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        return text\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "for column in ['query', 'title', 'description', 'summary']:\n",
    "    Labeled_Metadata[column] = Labeled_Metadata[column].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labeled_Metadata.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for classification\n",
    "X_cls = Labeled_Metadata['query'] + ' ' + Labeled_Metadata['title'] + ' ' + Labeled_Metadata['description'] + ' ' + Labeled_Metadata['summary']\n",
    "y_cls = Labeled_Metadata['label']\n",
    "X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(X_cls, y_cls, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text data for classification\n",
    "X_train_tokens_cls = tokenizer_cls(list(X_train_cls), padding=True, truncation=True, return_tensors='pt')\n",
    "X_test_tokens_cls = tokenizer_cls(list(X_test_cls), padding=True, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to tensors\n",
    "y_train_cls_tensor = torch.tensor(y_train_cls.values)\n",
    "y_test_cls_tensor = torch.tensor(y_test_cls.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tensor datasets\n",
    "train_dataset_cls = TensorDataset(X_train_tokens_cls.input_ids, X_train_tokens_cls.attention_mask, y_train_cls_tensor)\n",
    "test_dataset_cls = TensorDataset(X_test_tokens_cls.input_ids, X_test_tokens_cls.attention_mask, y_test_cls_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.TensorDataset object at 0x00000262AC29CAF0>\n",
      "<torch.utils.data.dataset.TensorDataset object at 0x00000262748A02E0>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(train_dataset_cls)\n",
    "print(test_dataset_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "batch_size = 8\n",
    "train_loader_cls = DataLoader(train_dataset_cls, batch_size=batch_size, shuffle=True)\n",
    "test_loader_cls = DataLoader(test_dataset_cls, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define optimizer and loss function\n",
    "learning_rate = 2e-5  \n",
    "optimizer_cls = AdamW(model_cls.parameters(), lr=learning_rate)\n",
    "criterion_cls = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Step 10/128, Loss: 0.17760060727596283\n",
      "Step 20/128, Loss: 0.08405087143182755\n",
      "Step 30/128, Loss: 0.45002347230911255\n",
      "Step 40/128, Loss: 0.058092888444662094\n",
      "Step 50/128, Loss: 0.09061025083065033\n",
      "Step 60/128, Loss: 0.4177345037460327\n",
      "Step 70/128, Loss: 0.0660485327243805\n",
      "Step 80/128, Loss: 0.44476690888404846\n",
      "Step 90/128, Loss: 0.07182550430297852\n",
      "Step 100/128, Loss: 0.3900478482246399\n",
      "Step 110/128, Loss: 0.4117822051048279\n",
      "Step 120/128, Loss: 0.07616524398326874\n",
      "Epoch 2/3\n",
      "Step 10/128, Loss: 0.056320562958717346\n",
      "Step 20/128, Loss: 0.07138118147850037\n",
      "Step 30/128, Loss: 1.1121132373809814\n",
      "Step 40/128, Loss: 0.05707618221640587\n",
      "Step 50/128, Loss: 0.7856000661849976\n",
      "Step 60/128, Loss: 0.41835907101631165\n",
      "Step 70/128, Loss: 0.40329042077064514\n",
      "Step 80/128, Loss: 0.3917740285396576\n",
      "Step 90/128, Loss: 0.7029182314872742\n",
      "Step 100/128, Loss: 0.12035848945379257\n",
      "Step 110/128, Loss: 0.09176396578550339\n",
      "Step 120/128, Loss: 0.3963022232055664\n",
      "Epoch 3/3\n",
      "Step 10/128, Loss: 0.044816646724939346\n",
      "Step 20/128, Loss: 0.7643232941627502\n",
      "Step 30/128, Loss: 0.06390620768070221\n"
     ]
    }
   ],
   "source": [
    "num_epochs=3\n",
    "for epoch in range(num_epochs):\n",
    "    model_cls.train()\n",
    "    total_loss = 0\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for step, batch in enumerate(train_loader_cls):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        optimizer_cls.zero_grad()\n",
    "        outputs_cls = model_cls(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss_cls = outputs_cls.loss\n",
    "        loss_cls.backward()\n",
    "        optimizer_cls.step()\n",
    "        \n",
    "        total_loss += loss_cls.item()\n",
    "        \n",
    "        if step % 10 == 0 and step > 0:\n",
    "            print(f\"Step {step}/{len(train_loader_cls)}, Loss: {loss_cls.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model_cls.eval()\n",
    "predictions_cls = []\n",
    "true_labels_cls = []\n",
    "for batch in test_loader_cls:\n",
    "    input_ids, attention_mask, labels = batch\n",
    "    with torch.no_grad():\n",
    "        outputs_cls = model_cls(input_ids, attention_mask=attention_mask)\n",
    "    logits_cls = outputs_cls.logits\n",
    "    preds_cls = torch.argmax(logits_cls, dim=1).tolist()\n",
    "    predictions_cls.extend(preds_cls)\n",
    "    true_labels_cls.extend(labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy: 0.9803921568627451\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy_cls = accuracy_score(true_labels_cls, predictions_cls)\n",
    "print(\"Classification Accuracy:\", accuracy_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity scores\n",
    "metadata_embeddings = []\n",
    "for index, row in Labeled_Metadata.iterrows():\n",
    "    text = row['title'] + ' ' + row['description'] + ' ' + row['summary']\n",
    "    tokenized_text = tokenizer_sim(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        output_sim = model_sim(**tokenized_text)\n",
    "    embeddings = output_sim.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    metadata_embeddings.append(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "electric cars\n",
      "Predicted Class: 0\n"
     ]
    }
   ],
   "source": [
    "user_input = input(\"Enter your query: \")\n",
    "\n",
    "# Tokenize user input for classification\n",
    "user_input_tokens_cls = tokenizer_cls(user_input, padding=True, truncation=True, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    output_cls = model_cls(**user_input_tokens_cls)\n",
    "predicted_class = torch.argmax(output_cls.logits).item()\n",
    "\n",
    "# Display predicted class\n",
    "print(user_input)\n",
    "print(\"Predicted Class:\", predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "\n",
    "# Random over-sampling\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imblearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, TensorDataset\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'imblearn'"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from \n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer for classification\n",
    "tokenizer_cls = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_cls = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  \n",
    "\n",
    "# Load sample dataset\n",
    "sample_data = Labeled_Metadata.copy()\n",
    "\n",
    "# Preprocess text data\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        return text\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "for column in ['query', 'title', 'description', 'summary']:\n",
    "    sample_data[column] = sample_data[column].apply(preprocess_text)\n",
    "\n",
    "# Split the data into features and labels\n",
    "X_cls = sample_data['query'] + ' ' + sample_data['title'] + ' ' + sample_data['description'] + ' ' + sample_data['summary']\n",
    "y_cls = sample_data['label']\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled_cls, y_resampled_cls = smote.fit_resample(X_cls, y_cls)\n",
    "\n",
    "# Split the resampled data into training and testing sets\n",
    "X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(X_resampled_cls, y_resampled_cls, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize text data for classification\n",
    "X_train_tokens_cls = tokenizer_cls(X_train_cls.tolist(), padding=True, truncation=True, return_tensors='pt')\n",
    "X_test_tokens_cls = tokenizer_cls(X_test_cls.tolist(), padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Create Tensor datasets\n",
    "train_dataset_cls = TensorDataset(X_train_tokens_cls.input_ids, X_train_tokens_cls.attention_mask, torch.tensor(y_train_cls))\n",
    "test_dataset_cls = TensorDataset(X_test_tokens_cls.input_ids, X_test_tokens_cls.attention_mask, torch.tensor(y_test_cls))\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 32  # Define your batch size\n",
    "train_loader_cls = DataLoader(train_dataset_cls, batch_size=batch_size, shuffle=True)\n",
    "test_loader_cls = DataLoader(test_dataset_cls, batch_size=batch_size)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer_cls = AdamW(model_cls.parameters(), lr=2e-5)  # Define your learning rate\n",
    "criterion_cls = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Fine-tune the model\n",
    "num_epochs = 5  # Define your number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model_cls.train()\n",
    "    for batch in train_loader_cls:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        optimizer_cls.zero_grad()\n",
    "        outputs_cls = model_cls(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss_cls = outputs_cls.loss\n",
    "        loss_cls.backward()\n",
    "        optimizer_cls.step()\n",
    "\n",
    "# Evaluate the model\n",
    "model_cls.eval()\n",
    "predictions_cls = []\n",
    "true_labels_cls = []\n",
    "for batch in test_loader_cls:\n",
    "    input_ids, attention_mask, labels = batch\n",
    "    with torch.no_grad():\n",
    "        outputs_cls = model_cls(input_ids, attention_mask=attention_mask)\n",
    "    logits_cls = outputs_cls.logits\n",
    "    preds_cls = torch.argmax(logits_cls, dim=1).tolist()\n",
    "    predictions_cls.extend(preds_cls)\n",
    "    true_labels_cls.extend(labels.tolist())\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_cls = accuracy_score(true_labels_cls, predictions_cls)\n",
    "print(\"Classification Accuracy:\", accuracy_cls)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
