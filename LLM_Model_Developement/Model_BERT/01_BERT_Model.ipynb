{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install required libraries if not already installed\n",
    "!pip install wordcloud\n",
    "!pip install seaborn\n",
    "!pip install sklearn\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages Installation\n",
    "import pandas as pd\n",
    "from transformers import Trainer, TrainingArguments, T5ForConditionalGeneration, T5Tokenizer\n",
    "from datasets import load_metric\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fetching Metadata from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "Metadata = pd.read_csv('C:\\\\24592774_Dataset_Discovery_Using_LLM\\\\MetaData_Creation\\\\MetaData_Notebooks\\\\Enriched_MetaData_DataSet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>summary</th>\n",
       "      <th>tags</th>\n",
       "      <th>dataset_url</th>\n",
       "      <th>available_formats</th>\n",
       "      <th>initial_tags</th>\n",
       "      <th>augmented_tags</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>final_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['2023w7', 'global', 'electric', 'vehicle', 'm...</td>\n",
       "      <td>['global', 'electric', 'vehicle', 'market', 's...</td>\n",
       "      <td>['editorsimple', 'original', 'visualization', ...</td>\n",
       "      <td>['makeover monday', 'cars', 'vehicles', 'elect...</td>\n",
       "      <td>https://data.world/makeovermonday/2023w7</td>\n",
       "      <td>['xlsx']</td>\n",
       "      <td>source, data, original, vehicle, electric</td>\n",
       "      <td>original, electric, source, data, vehicle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['original', ' electric', ' source', ' ', ' da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['connected', 'electric', 'autonomous', 'vehic...</td>\n",
       "      <td>['connected', 'electric', 'autonomous', 'vehic...</td>\n",
       "      <td>['rolling', 'snapshot', 'data', 'collected', '...</td>\n",
       "      <td>['autonomous']</td>\n",
       "      <td>https://data.world/smartcolumbusos/650b7e59-af...</td>\n",
       "      <td>['csv']</td>\n",
       "      <td>updated, available, including, vehicle, collec...</td>\n",
       "      <td>electric, source, data, available, including, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['electric', ' ', ' source', ' data', ' availa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['electric', 'vehicle', 'charging', 'stations']</td>\n",
       "      <td>['dataset', 'contains', 'session', 'details', ...</td>\n",
       "      <td>['dataset', 'contains', 'session', 'details', ...</td>\n",
       "      <td>['electric vehicle', 'environment', 'energy']</td>\n",
       "      <td>https://data.world/townofcary/electric-vehicle...</td>\n",
       "      <td>['dbf', 'csv', 'shx', 'shp', 'json', 'prj']</td>\n",
       "      <td>dataset, charging, updated, available, source,...</td>\n",
       "      <td>electric, source, available, dataset, vehicle,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['electric', ' ', ' source', ' available', ' d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['nyserda', 'electric', 'vehicle', 'drive', 'c...</td>\n",
       "      <td>['new', 'york', 'state', '’', 'charge', 'ny', ...</td>\n",
       "      <td>['original', 'title', 'nyserda', 'electric', '...</td>\n",
       "      <td>['ev', 'electric vehicle', 'bev', 'phev', 'ghg...</td>\n",
       "      <td>https://data.world/data-ny-gov/thd2-fu8y</td>\n",
       "      <td>['csv']</td>\n",
       "      <td>service, using, use, information, research, sa...</td>\n",
       "      <td>original, electric, new, source, research, use...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['original', ' electric', ' new', ' source', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['impact', 'uncoordinated', 'plugin', 'electri...</td>\n",
       "      <td>['impact', 'uncoordinated', 'plugin', 'electri...</td>\n",
       "      <td>['original', 'title', 'impact', 'uncoordinated...</td>\n",
       "      <td>['battery', 'consumption', 'data', 'energy', '...</td>\n",
       "      <td>https://data.world/us-doe-gov/8ae7e117-313b-40...</td>\n",
       "      <td>['xlsx']</td>\n",
       "      <td>total, number, al, united, using, use, chargin...</td>\n",
       "      <td>original, number, source, electric, use, data,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['original', ' number', ' source', ' electric'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  ['2023w7', 'global', 'electric', 'vehicle', 'm...   \n",
       "1  ['connected', 'electric', 'autonomous', 'vehic...   \n",
       "2    ['electric', 'vehicle', 'charging', 'stations']   \n",
       "3  ['nyserda', 'electric', 'vehicle', 'drive', 'c...   \n",
       "4  ['impact', 'uncoordinated', 'plugin', 'electri...   \n",
       "\n",
       "                                         description  \\\n",
       "0  ['global', 'electric', 'vehicle', 'market', 's...   \n",
       "1  ['connected', 'electric', 'autonomous', 'vehic...   \n",
       "2  ['dataset', 'contains', 'session', 'details', ...   \n",
       "3  ['new', 'york', 'state', '’', 'charge', 'ny', ...   \n",
       "4  ['impact', 'uncoordinated', 'plugin', 'electri...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  ['editorsimple', 'original', 'visualization', ...   \n",
       "1  ['rolling', 'snapshot', 'data', 'collected', '...   \n",
       "2  ['dataset', 'contains', 'session', 'details', ...   \n",
       "3  ['original', 'title', 'nyserda', 'electric', '...   \n",
       "4  ['original', 'title', 'impact', 'uncoordinated...   \n",
       "\n",
       "                                                tags  \\\n",
       "0  ['makeover monday', 'cars', 'vehicles', 'elect...   \n",
       "1                                     ['autonomous']   \n",
       "2      ['electric vehicle', 'environment', 'energy']   \n",
       "3  ['ev', 'electric vehicle', 'bev', 'phev', 'ghg...   \n",
       "4  ['battery', 'consumption', 'data', 'energy', '...   \n",
       "\n",
       "                                         dataset_url  \\\n",
       "0           https://data.world/makeovermonday/2023w7   \n",
       "1  https://data.world/smartcolumbusos/650b7e59-af...   \n",
       "2  https://data.world/townofcary/electric-vehicle...   \n",
       "3           https://data.world/data-ny-gov/thd2-fu8y   \n",
       "4  https://data.world/us-doe-gov/8ae7e117-313b-40...   \n",
       "\n",
       "                             available_formats  \\\n",
       "0                                     ['xlsx']   \n",
       "1                                      ['csv']   \n",
       "2  ['dbf', 'csv', 'shx', 'shp', 'json', 'prj']   \n",
       "3                                      ['csv']   \n",
       "4                                     ['xlsx']   \n",
       "\n",
       "                                        initial_tags  \\\n",
       "0          source, data, original, vehicle, electric   \n",
       "1  updated, available, including, vehicle, collec...   \n",
       "2  dataset, charging, updated, available, source,...   \n",
       "3  service, using, use, information, research, sa...   \n",
       "4  total, number, al, united, using, use, chargin...   \n",
       "\n",
       "                                      augmented_tags  ner_tags  \\\n",
       "0          original, electric, source, data, vehicle       NaN   \n",
       "1  electric, source, data, available, including, ...       NaN   \n",
       "2  electric, source, available, dataset, vehicle,...       NaN   \n",
       "3  original, electric, new, source, research, use...       NaN   \n",
       "4  original, number, source, electric, use, data,...       NaN   \n",
       "\n",
       "                                          final_tags  \n",
       "0  ['original', ' electric', ' source', ' ', ' da...  \n",
       "1  ['electric', ' ', ' source', ' data', ' availa...  \n",
       "2  ['electric', ' ', ' source', ' available', ' d...  \n",
       "3  ['original', ' electric', ' new', ' source', '...  \n",
       "4  ['original', ' number', ' source', ' electric'...  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                 object\n",
       "description           object\n",
       "summary               object\n",
       "tags                  object\n",
       "dataset_url           object\n",
       "available_formats     object\n",
       "initial_tags          object\n",
       "augmented_tags        object\n",
       "ner_tags             float64\n",
       "final_tags            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MetaData_DS.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Average Training Loss: 0.3679824247956276\n",
      "Epoch 1/3, Training Accuracy: 0.9841269841269841\n",
      "Epoch 2/3, Average Training Loss: 0.1431718897074461\n",
      "Epoch 2/3, Training Accuracy: 1.0\n",
      "Epoch 3/3, Average Training Loss: 0.066731839440763\n",
      "Epoch 3/3, Training Accuracy: 1.0\n",
      "Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0\n",
      "Validation Recall: 1.0\n",
      "Validation F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertModel, AdamW\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "MetaData_DS = Metadata.copy()\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "max_length = 128  \n",
    "\n",
    "# Tokenize dataset descriptions\n",
    "tokenized_descriptions = tokenizer(list(MetaData_DS['description']), truncation=True, padding=True, max_length=max_length)\n",
    "\n",
    "# Convert labels to numerical format\n",
    "# For example, if you have binary labels (0 or 1)\n",
    "labels = [1 if url else 0 for url in MetaData_DS['dataset_url']]\n",
    "\n",
    "# Extract input_ids and attention_mask\n",
    "input_ids = tokenized_descriptions['input_ids']\n",
    "attention_mask = tokenized_descriptions['attention_mask']\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_ids, val_ids, train_masks, val_masks, train_labels, val_labels = train_test_split(input_ids, attention_mask, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert tokenized texts and labels to PyTorch tensors\n",
    "train_ids = torch.tensor(train_ids)\n",
    "val_ids = torch.tensor(val_ids)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "# Create TensorDataset for training and validation sets\n",
    "train_dataset = TensorDataset(train_ids, train_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_ids, val_masks, val_labels)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Create DataLoader for training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the BERT-based model\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.linear = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.linear(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Instantiate the BERT model\n",
    "num_classes = 2 \n",
    "model = BERTClassifier(num_classes)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for batch in train_loader:\n",
    "        inputs, attention_masks, labels = batch\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs, attention_masks)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy for this batch\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    # Print average training loss for the epoch\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Average Training Loss: {total_loss / len(train_loader)}\")\n",
    "    # Print training accuracy for the epoch\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Training Accuracy: {total_correct / total_samples}\")\n",
    "\n",
    "# Validation loop\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        inputs, attention_masks, labels = batch\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs, attention_masks)\n",
    "        \n",
    "        # Compute predictions\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "        # Update total samples and total correct predictions\n",
    "        total_samples += labels.size(0)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Save predictions and labels for metric calculation\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Compute validation accuracy\n",
    "accuracy = total_correct / total_samples\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "\n",
    "# Compute other metrics\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='binary')\n",
    "print(f\"Validation Precision: {precision}\")\n",
    "print(f\"Validation Recall: {recall}\")\n",
    "print(f\"Validation F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model seems to be overfitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get BERT embeddings\n",
    "def get_bert_embedding(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.bert(**inputs)\n",
    "    return outputs.pooler_output\n",
    "\n",
    "# Get BERT embeddings for all dataset descriptions\n",
    "dataset_embeddings = [get_bert_embedding(desc, model, tokenizer) for desc in MetaData_DS['description']]\n",
    "dataset_embeddings = torch.cat(dataset_embeddings, dim=0)\n",
    "\n",
    "# Function to compute cosine similarity\n",
    "def cosine_similarity(emb1, emb2):\n",
    "    return F.cosine_similarity(emb1, emb2).mean().item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ['gpm', 'ground', 'validation', 'twodimensional', 'video', 'disdromete', 'v1']\n",
      "Dataset URL: https://data.world/us-nasa-gov/47e3817a-65aa-4af4-8d40-f0ed0f655bda\n",
      "Cosine Similarity: 0.8438868522644043\n",
      "\n",
      "Title: ['cars', 'purchased', 'vehicles', 'xls']\n",
      "Dataset URL: https://data.world/nhtsa/cars-purchased-vehicles-xls\n",
      "Cosine Similarity: 0.8458564877510071\n",
      "\n",
      "Title: ['iptv', 'subscription', 'service', 'go']\n",
      "Dataset URL: https://data.world/freemotion/which-iptv-subscription-service-should-you-go-for\n",
      "Cosine Similarity: 0.8654544353485107\n",
      "\n",
      "Title: ['recognized', 'sports']\n",
      "Dataset URL: https://data.world/sports/recognized-sports\n",
      "Cosine Similarity: 0.8654544353485107\n",
      "\n",
      "Title: ['sports', 'illustrated', 'covers']\n",
      "Dataset URL: https://data.world/crowdflower/sports-illustrated-covers\n",
      "Cosine Similarity: 0.8654544353485107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to predict dataset URLs based on search text\n",
    "def predict_dataset_url(search_text):\n",
    "    search_emb = get_bert_embedding(search_text, model, tokenizer)\n",
    "    similarities = [cosine_similarity(search_emb, dataset_emb) for dataset_emb in dataset_embeddings]\n",
    "    \n",
    "    # Find the maximum and minimum similarity values\n",
    "    max_similarity = max(similarities)\n",
    "    min_similarity = min(similarities)\n",
    "    \n",
    "    # Calculate the 90% range\n",
    "    range_90 = 0.90 * (max_similarity - min_similarity)\n",
    "    \n",
    "    # Find the threshold value\n",
    "    threshold = min_similarity + range_90\n",
    "    \n",
    "    # Find the indices of datasets with similarity above the threshold\n",
    "    similar_indices = [i for i, sim in enumerate(similarities) if sim > threshold]\n",
    "    \n",
    "    results = []\n",
    "    for idx in similar_indices:\n",
    "        dataset_info = {\n",
    "            \"title\": MetaData_DS.iloc[idx]['title'],\n",
    "            \"dataset_url\": MetaData_DS.iloc[idx]['dataset_url'],\n",
    "            \"cosine_similarity\": similarities[idx]\n",
    "        }\n",
    "        results.append(dataset_info)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "search_text = \"electric vehicles\"\n",
    "results = predict_dataset_url(search_text)\n",
    "\n",
    "# Display results\n",
    "if not results:\n",
    "    print(\"No datasets found above the 90% threshold.\")\n",
    "else:\n",
    "    for result in results:\n",
    "        print(f\"Title: {result['title']}\")\n",
    "        print(f\"Dataset URL: {result['dataset_url']}\")\n",
    "        print(f\"Cosine Similarity: {result['cosine_similarity']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Average Training Loss: 2.9691543579101562\n",
      "Epoch 1/3, Training Accuracy: 0.8412698412698413\n",
      "Epoch 2/3, Average Training Loss: 2.6702473759651184\n",
      "Epoch 2/3, Training Accuracy: 1.0\n",
      "Epoch 3/3, Average Training Loss: 2.6028189063072205\n",
      "Epoch 3/3, Training Accuracy: 1.0\n",
      "Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0\n",
      "Validation Recall: 1.0\n",
      "Validation F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertModel, AdamW\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "MetaData_DS = Metadata.copy()\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "max_length = 128  \n",
    "\n",
    "# Tokenize dataset descriptions\n",
    "tokenized_descriptions = tokenizer(list(MetaData_DS['description']), truncation=True, padding=True, max_length=max_length)\n",
    "\n",
    "# Convert labels to numerical format\n",
    "# For example, if you have binary labels (0 or 1)\n",
    "labels = [1 if url else 0 for url in MetaData_DS['dataset_url']]\n",
    "\n",
    "# Extract input_ids and attention_mask\n",
    "input_ids = tokenized_descriptions['input_ids']\n",
    "attention_mask = tokenized_descriptions['attention_mask']\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_ids, val_ids, train_masks, val_masks, train_labels, val_labels = train_test_split(input_ids, attention_mask, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert tokenized texts and labels to PyTorch tensors\n",
    "train_ids = torch.tensor(train_ids)\n",
    "val_ids = torch.tensor(val_ids)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "# Create TensorDataset for training and validation sets\n",
    "train_dataset = TensorDataset(train_ids, train_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_ids, val_masks, val_labels)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Create DataLoader for training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the BERT-based model with dropout and L2 regularization\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.1)  # Add dropout layer\n",
    "        self.linear = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "        self.regularization = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size)  # Add L2 regularization layer\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.linear(pooled_output)\n",
    "        # Add L2 regularization\n",
    "        regularization_loss = torch.sum(torch.square(self.regularization.weight))\n",
    "        return logits, regularization_loss\n",
    "\n",
    "# Instantiate the BERT model\n",
    "num_classes = 2 \n",
    "model = BERTClassifier(num_classes)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for batch in train_loader:\n",
    "        inputs, attention_masks, labels = batch\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs, regularization_loss = model(inputs, attention_masks)\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss += 0.01 * regularization_loss  # Add regularization loss to the total loss\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy for this batch\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    # Print average training loss for the epoch\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Average Training Loss: {total_loss / len(train_loader)}\")\n",
    "    # Print training accuracy for the epoch\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Training Accuracy: {total_correct / total_samples}\")\n",
    "\n",
    "# Validation loop\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        inputs, attention_masks, labels = batch\n",
    "\n",
    "        # Forward pass\n",
    "        outputs, _ = model(inputs, attention_masks)\n",
    "        \n",
    "        # Compute predictions\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "        # Update total samples and total correct predictions\n",
    "        total_samples += labels.size(0)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Save predictions and labels for metric calculation\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Compute validation accuracy\n",
    "accuracy = total_correct / total_samples\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "\n",
    "# Compute other metrics\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='binary')\n",
    "print(f\"Validation Precision: {precision}\")\n",
    "print(f\"Validation Recall: {recall}\")\n",
    "print(f\"Validation F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Average Training Loss: 3.144028604030609\n",
      "Epoch 1/3, Training Accuracy: 0.746031746031746\n",
      "Epoch 2/3, Average Training Loss: 2.8386403918266296\n",
      "Epoch 2/3, Training Accuracy: 1.0\n",
      "Epoch 3/3, Average Training Loss: 2.667064368724823\n",
      "Epoch 3/3, Training Accuracy: 1.0\n",
      "Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0\n",
      "Validation Recall: 1.0\n",
      "Validation F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertModel, AdamW\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "MetaData_DS = Metadata.copy()\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "max_length = 128  \n",
    "\n",
    "# Tokenize dataset descriptions\n",
    "tokenized_descriptions = tokenizer(list(MetaData_DS['description']), truncation=True, padding=True, max_length=max_length)\n",
    "\n",
    "# Convert labels to numerical format\n",
    "# For example, if you have binary labels (0 or 1)\n",
    "labels = [1 if url else 0 for url in MetaData_DS['dataset_url']]\n",
    "\n",
    "# Extract input_ids and attention_mask\n",
    "input_ids = tokenized_descriptions['input_ids']\n",
    "attention_mask = tokenized_descriptions['attention_mask']\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_ids, val_ids, train_masks, val_masks, train_labels, val_labels = train_test_split(input_ids, attention_mask, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert tokenized texts and labels to PyTorch tensors\n",
    "train_ids = torch.tensor(train_ids)\n",
    "val_ids = torch.tensor(val_ids)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "# Create TensorDataset for training and validation sets\n",
    "train_dataset = TensorDataset(train_ids, train_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_ids, val_masks, val_labels)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Create DataLoader for training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the BERT-based model with dropout and L2 regularization\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.1)  # Add dropout layer\n",
    "        self.linear = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "        self.regularization = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size)  # Add L2 regularization layer\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.linear(pooled_output)\n",
    "        # Add L2 regularization\n",
    "        regularization_loss = torch.sum(torch.square(self.regularization.weight))\n",
    "        return logits, regularization_loss\n",
    "\n",
    "# Instantiate the BERT model\n",
    "num_classes = 2  \n",
    "model = BERTClassifier(num_classes)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for batch in train_loader:\n",
    "        inputs, attention_masks, labels = batch\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs, regularization_loss = model(inputs, attention_masks)\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss += 0.01 * regularization_loss  # Add regularization loss to the total loss\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy for this batch\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    # Print average training loss for the epoch\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Average Training Loss: {total_loss / len(train_loader)}\")\n",
    "    # Print training accuracy for the epoch\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Training Accuracy: {total_correct / total_samples}\")\n",
    "\n",
    "# Validation loop\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        inputs, attention_masks, labels = batch\n",
    "\n",
    "        # Forward pass\n",
    "        outputs, _ = model(inputs, attention_masks)\n",
    "        \n",
    "        # Compute predictions\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "        # Update total samples and total correct predictions\n",
    "        total_samples += labels.size(0)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Save predictions and labels for metric calculation\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Compute validation accuracy\n",
    "accuracy = total_correct / total_samples\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "\n",
    "# Compute other metrics\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='binary')\n",
    "print(f\"Validation Precision: {precision}\")\n",
    "print(f\"Validation Recall: {recall}\")\n",
    "print(f\"Validation F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 0.5783726349473, Train Accuracy: 0.7301587301587301\n",
      "Validation Loss: 0.3101840615272522, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 2/10\n",
      "Train Loss: 0.2463531270623207, Train Accuracy: 1.0\n",
      "Validation Loss: 0.15873892605304718, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 3/10\n",
      "Train Loss: 0.14758636429905891, Train Accuracy: 1.0\n",
      "Validation Loss: 0.08207385241985321, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 4/10\n",
      "Train Loss: 0.07775966823101044, Train Accuracy: 1.0\n",
      "Validation Loss: 0.04271232709288597, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 5/10\n",
      "Train Loss: 0.041524085216224194, Train Accuracy: 1.0\n",
      "Validation Loss: 0.023756233975291252, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 6/10\n",
      "Train Loss: 0.024783104192465544, Train Accuracy: 1.0\n",
      "Validation Loss: 0.01291266456246376, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 7/10\n",
      "Train Loss: 0.0141671197488904, Train Accuracy: 1.0\n",
      "Validation Loss: 0.0077364384196698666, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 8/10\n",
      "Train Loss: 0.009109260747209191, Train Accuracy: 1.0\n",
      "Validation Loss: 0.006267812568694353, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 9/10\n",
      "Train Loss: 0.007179188192822039, Train Accuracy: 1.0\n",
      "Validation Loss: 0.004419195931404829, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 10/10\n",
      "Train Loss: 0.005617538234218955, Train Accuracy: 1.0\n",
      "Validation Loss: 0.0034717849921435118, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertModel\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "MetaData_DS = Metadata.copy()\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "max_length = 128 \n",
    "\n",
    "# Tokenize dataset descriptions\n",
    "tokenized_descriptions = tokenizer(list(MetaData_DS['description']), truncation=True, padding=True, max_length=max_length)\n",
    "\n",
    "# Convert labels to numerical format\n",
    "labels = [1 if url else 0 for url in MetaData_DS['dataset_url']]\n",
    "\n",
    "# Extract input_ids and attention_mask\n",
    "input_ids = tokenized_descriptions['input_ids']\n",
    "attention_mask = tokenized_descriptions['attention_mask']\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_ids, val_ids, train_masks, val_masks, train_labels, val_labels = train_test_split(input_ids, attention_mask, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert tokenized texts and labels to PyTorch tensors\n",
    "train_ids = torch.tensor(train_ids)\n",
    "val_ids = torch.tensor(val_ids)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "# Create TensorDataset for training and validation sets\n",
    "train_dataset = TensorDataset(train_ids, train_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_ids, val_masks, val_labels)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Create DataLoader for training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the BERT-based model with dropout and L2 regularization\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.3)  # Increase dropout rate\n",
    "        self.linear = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.linear(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Instantiate the BERT model\n",
    "num_classes = 2  \n",
    "model = BERTClassifier(num_classes)\n",
    "\n",
    "# Define the optimizer, scheduler and loss function\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=1, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop with early stopping\n",
    "epochs = 10\n",
    "patience = 2\n",
    "best_loss = float('inf')\n",
    "trigger_times = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for batch in train_loader:\n",
    "        inputs, attention_masks, labels = batch\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs, attention_masks)\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy for this batch\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = total_correct / total_samples\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, attention_masks, labels = batch\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs, attention_masks)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Compute predictions\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "            # Update total samples and total correct predictions\n",
    "            total_samples += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Save predictions and labels for metric calculation\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='binary')\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    print(f\"Train Loss: {avg_train_loss}, Train Accuracy: {train_accuracy}\")\n",
    "    print(f\"Validation Loss: {avg_val_loss}, Validation Accuracy: {accuracy}\")\n",
    "    print(f\"Validation Precision: {precision}, Validation Recall: {recall}, Validation F1 Score: {f1}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        trigger_times = 0\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print('Early stopping!')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 0.4851943999528885, Train Accuracy: 0.9047619047619048\n",
      "Validation Loss: 0.2962067127227783, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 2/10\n",
      "Train Loss: 0.30162081867456436, Train Accuracy: 1.0\n",
      "Validation Loss: 0.1883525401353836, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 3/10\n",
      "Train Loss: 0.18902996182441711, Train Accuracy: 1.0\n",
      "Validation Loss: 0.11559051275253296, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 4/10\n",
      "Train Loss: 0.11456568539142609, Train Accuracy: 1.0\n",
      "Validation Loss: 0.06862010806798935, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 5/10\n",
      "Train Loss: 0.07565614953637123, Train Accuracy: 1.0\n",
      "Validation Loss: 0.03835596516728401, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 6/10\n",
      "Train Loss: 0.0465706642717123, Train Accuracy: 1.0\n",
      "Validation Loss: 0.021881263703107834, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 7/10\n",
      "Train Loss: 0.031378640327602625, Train Accuracy: 1.0\n",
      "Validation Loss: 0.014299692586064339, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 8/10\n",
      "Train Loss: 0.02264755731448531, Train Accuracy: 1.0\n",
      "Validation Loss: 0.01043669879436493, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 9/10\n",
      "Train Loss: 0.015855712117627263, Train Accuracy: 1.0\n",
      "Validation Loss: 0.007946038618683815, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 10/10\n",
      "Train Loss: 0.012435249984264374, Train Accuracy: 1.0\n",
      "Validation Loss: 0.006265504285693169, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertModel\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "MetaData_DS = Metadata.copy()\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "max_length = 128  \n",
    "\n",
    "# Tokenize dataset descriptions\n",
    "tokenized_descriptions = tokenizer(list(MetaData_DS['description']), truncation=True, padding=True, max_length=max_length)\n",
    "\n",
    "# Convert labels to numerical format\n",
    "labels = [1 if url else 0 for url in MetaData_DS['dataset_url']]\n",
    "\n",
    "# Extract input_ids and attention_mask\n",
    "input_ids = tokenized_descriptions['input_ids']\n",
    "attention_mask = tokenized_descriptions['attention_mask']\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_ids, val_ids, train_masks, val_masks, train_labels, val_labels = train_test_split(input_ids, attention_mask, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert tokenized texts and labels to PyTorch tensors\n",
    "train_ids = torch.tensor(train_ids)\n",
    "val_ids = torch.tensor(val_ids)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "# Create TensorDataset for training and validation sets\n",
    "train_dataset = TensorDataset(train_ids, train_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_ids, val_masks, val_labels)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Create DataLoader for training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the BERT-based model with dropout and L2 regularization\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased', hidden_dropout_prob=0.3, attention_probs_dropout_prob=0.3) \n",
    "        self.dropout = nn.Dropout(0.3)  # Increase dropout rate\n",
    "        self.linear = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.linear(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Instantiate the BERT model\n",
    "num_classes = 2  \n",
    "model = BERTClassifier(num_classes)\n",
    "\n",
    "# Define the optimizer with weight decay\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)  # Add weight decay\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=1, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop with early stopping\n",
    "epochs = 10\n",
    "patience = 2\n",
    "best_loss = float('inf')\n",
    "trigger_times = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for batch in train_loader:\n",
    "        inputs, attention_masks, labels = batch\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs, attention_masks)\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy for this batch\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = total_correct / total_samples\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, attention_masks, labels = batch\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs, attention_masks)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Compute predictions\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "            # Update total samples and total correct predictions\n",
    "            total_samples += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Save predictions and labels for metric calculation\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='binary')\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    print(f\"Train Loss: {avg_train_loss}, Train Accuracy: {train_accuracy}\")\n",
    "    print(f\"Validation Loss: {avg_val_loss}, Validation Accuracy: {accuracy}\")\n",
    "    print(f\"Validation Precision: {precision}, Validation Recall: {recall}, Validation F1 Score: {f1}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        trigger_times = 0\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print('Early stopping!')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 0.8908563852310181, Train Accuracy: 0.2698412698412698\n",
      "Validation Loss: 0.7927955389022827, Validation Accuracy: 0.125\n",
      "Validation Precision: 1.0, Validation Recall: 0.125, Validation F1 Score: 0.2222222222222222\n",
      "Epoch 2/10\n",
      "Train Loss: 0.39846061170101166, Train Accuracy: 0.9523809523809523\n",
      "Validation Loss: 0.4657982289791107, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 3/10\n",
      "Train Loss: 0.28323177620768547, Train Accuracy: 1.0\n",
      "Validation Loss: 0.25540706515312195, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 4/10\n",
      "Train Loss: 0.19210970029234886, Train Accuracy: 1.0\n",
      "Validation Loss: 0.17330004274845123, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 5/10\n",
      "Train Loss: 0.15426048263907433, Train Accuracy: 1.0\n",
      "Validation Loss: 0.13986526429653168, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 6/10\n",
      "Train Loss: 0.11009392514824867, Train Accuracy: 1.0\n",
      "Validation Loss: 0.12127979099750519, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 7/10\n",
      "Train Loss: 0.09066287893801928, Train Accuracy: 0.9841269841269841\n",
      "Validation Loss: 0.09736385941505432, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 8/10\n",
      "Train Loss: 0.06309087760746479, Train Accuracy: 1.0\n",
      "Validation Loss: 0.05959724634885788, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 9/10\n",
      "Train Loss: 0.034507475793361664, Train Accuracy: 1.0\n",
      "Validation Loss: 0.0297046210616827, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 10/10\n",
      "Train Loss: 0.023997437208890915, Train Accuracy: 1.0\n",
      "Validation Loss: 0.015901468694210052, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertModel\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "MetaData_DS = Metadata.copy()\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "max_length = 128  \n",
    "\n",
    "# Tokenize dataset descriptions\n",
    "tokenized_descriptions = tokenizer(list(MetaData_DS['description']), truncation=True, padding=True, max_length=max_length)\n",
    "\n",
    "# Convert labels to numerical format\n",
    "labels = [1 if url else 0 for url in MetaData_DS['dataset_url']]\n",
    "\n",
    "# Extract input_ids and attention_mask\n",
    "input_ids = tokenized_descriptions['input_ids']\n",
    "attention_mask = tokenized_descriptions['attention_mask']\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_ids, val_ids, train_masks, val_masks, train_labels, val_labels = train_test_split(input_ids, attention_mask, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert tokenized texts and labels to PyTorch tensors\n",
    "train_ids = torch.tensor(train_ids)\n",
    "val_ids = torch.tensor(val_ids)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "# Create TensorDataset for training and validation sets\n",
    "train_dataset = TensorDataset(train_ids, train_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_ids, val_masks, val_labels)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Create DataLoader for training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the BERT-based model with increased dropout\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased', hidden_dropout_prob=0.5, attention_probs_dropout_prob=0.5)  # Further increased BERT dropout\n",
    "        self.dropout = nn.Dropout(0.5)  # Further increased dropout rate\n",
    "        self.linear = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.linear(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Instantiate the BERT model\n",
    "num_classes = 2 \n",
    "model = BERTClassifier(num_classes)\n",
    "\n",
    "# Define the optimizer with weight decay and gradient clipping\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=1, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop with early stopping\n",
    "epochs = 10\n",
    "patience = 2\n",
    "best_loss = float('inf')\n",
    "trigger_times = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for batch in train_loader:\n",
    "        inputs, attention_masks, labels = batch\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs, attention_masks)\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy for this batch\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = total_correct / total_samples\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, attention_masks, labels = batch\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs, attention_masks)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Compute predictions\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "            # Update total samples and total correct predictions\n",
    "            total_samples += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Save predictions and labels for metric calculation\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='binary')\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    print(f\"Train Loss: {avg_train_loss}, Train Accuracy: {train_accuracy}\")\n",
    "    print(f\"Validation Loss: {avg_val_loss}, Validation Accuracy: {accuracy}\")\n",
    "    print(f\"Validation Precision: {precision}, Validation Recall: {recall}, Validation F1 Score: {f1}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        trigger_times = 0\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print('Early stopping!')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 0.607300654053688, Train Accuracy: 0.6825396825396826\n",
      "Validation Loss: 0.6546359062194824, Validation Accuracy: 0.625\n",
      "Validation Precision: 1.0, Validation Recall: 0.625, Validation F1 Score: 0.7692307692307693\n",
      "Epoch 2/10\n",
      "Train Loss: 0.39308468997478485, Train Accuracy: 0.9365079365079365\n",
      "Validation Loss: 0.3837055265903473, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 3/10\n",
      "Train Loss: 0.25504642724990845, Train Accuracy: 0.9841269841269841\n",
      "Validation Loss: 0.24229596555233002, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 4/10\n",
      "Train Loss: 0.17578090727329254, Train Accuracy: 1.0\n",
      "Validation Loss: 0.17643404006958008, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 5/10\n",
      "Train Loss: 0.12218410149216652, Train Accuracy: 1.0\n",
      "Validation Loss: 0.13565519452095032, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 6/10\n",
      "Train Loss: 0.07404926232993603, Train Accuracy: 1.0\n",
      "Validation Loss: 0.09258756041526794, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 7/10\n",
      "Train Loss: 0.06760538555681705, Train Accuracy: 1.0\n",
      "Validation Loss: 0.0630902573466301, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 8/10\n",
      "Train Loss: 0.03939200611785054, Train Accuracy: 1.0\n",
      "Validation Loss: 0.03902607038617134, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 9/10\n",
      "Train Loss: 0.026780308224260807, Train Accuracy: 1.0\n",
      "Validation Loss: 0.022786328569054604, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 10/10\n",
      "Train Loss: 0.017159729730337858, Train Accuracy: 1.0\n",
      "Validation Loss: 0.016048524528741837, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertModel\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "MetaData_DS = Metadata.copy()\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "max_length = 128  \n",
    "MetaData_DS['combined_text'] = MetaData_DS['title'].astype(str) + ' ' + \\\n",
    "                               MetaData_DS['description'].astype(str) + ' ' + \\\n",
    "                               MetaData_DS['summary'].astype(str) + ' ' + \\\n",
    "                               MetaData_DS['augmented_tags'].astype(str)\n",
    "\n",
    "# Tokenize dataset descriptions\n",
    "tokenized_text = tokenizer(list(MetaData_DS['combined_text']), truncation=True, padding=True, max_length=max_length)\n",
    "\n",
    "# Convert labels to numerical format\n",
    "labels = [1 if url else 0 for url in MetaData_DS['dataset_url']]\n",
    "\n",
    "# Extract input_ids and attention_mask\n",
    "input_ids = tokenized_descriptions['input_ids']\n",
    "attention_mask = tokenized_descriptions['attention_mask']\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_ids, val_ids, train_masks, val_masks, train_labels, val_labels = train_test_split(input_ids, attention_mask, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert tokenized texts and labels to PyTorch tensors\n",
    "train_ids = torch.tensor(train_ids)\n",
    "val_ids = torch.tensor(val_ids)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "# Create TensorDataset for training and validation sets\n",
    "train_dataset = TensorDataset(train_ids, train_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_ids, val_masks, val_labels)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Create DataLoader for training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the BERT-based model with increased dropout\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased', hidden_dropout_prob=0.5, attention_probs_dropout_prob=0.5)  # Further increased BERT dropout\n",
    "        self.dropout = nn.Dropout(0.5)  # Further increased dropout rate\n",
    "        self.linear = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.linear(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Instantiate the BERT model\n",
    "num_classes = 2  \n",
    "model = BERTClassifier(num_classes)\n",
    "\n",
    "# Define the optimizer with weight decay and gradient clipping\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=1, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop with early stopping\n",
    "epochs = 10\n",
    "patience = 2\n",
    "best_loss = float('inf')\n",
    "trigger_times = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for batch in train_loader:\n",
    "        inputs, attention_masks, labels = batch\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs, attention_masks)\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy for this batch\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = total_correct / total_samples\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, attention_masks, labels = batch\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs, attention_masks)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Compute predictions\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "            # Update total samples and total correct predictions\n",
    "            total_samples += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Save predictions and labels for metric calculation\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='binary')\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    print(f\"Train Loss: {avg_train_loss}, Train Accuracy: {train_accuracy}\")\n",
    "    print(f\"Validation Loss: {avg_val_loss}, Validation Accuracy: {accuracy}\")\n",
    "    print(f\"Validation Precision: {precision}, Validation Recall: {recall}, Validation F1 Score: {f1}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        trigger_times = 0\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print('Early stopping!')\n",
    "            break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
