{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Packages Installation\n",
    "import pandas as pd\n",
    "from transformers import Trainer, TrainingArguments, T5ForConditionalGeneration, T5Tokenizer\n",
    "from datasets import load_metric\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Metadata = pd.read_csv('C:\\\\24592774_Dataset_Discovery_Using_LLM\\\\MetaData_Creation\\\\MetaData_Notebooks\\\\Prepared_MetaData_DataSet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>summary</th>\n",
       "      <th>tags</th>\n",
       "      <th>dataset_url</th>\n",
       "      <th>available_formats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['2023w7', 'global', 'electric', 'vehicle', 'm...</td>\n",
       "      <td>['global', 'electric', 'vehicle', 'market', 's...</td>\n",
       "      <td>['editorsimple', 'original', 'visualization', ...</td>\n",
       "      <td>['makeover monday', 'cars', 'vehicles', 'elect...</td>\n",
       "      <td>https://data.world/makeovermonday/2023w7</td>\n",
       "      <td>['xlsx']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['connected', 'electric', 'autonomous', 'vehic...</td>\n",
       "      <td>['connected', 'electric', 'autonomous', 'vehic...</td>\n",
       "      <td>['rolling', 'snapshot', 'data', 'collected', '...</td>\n",
       "      <td>['autonomous']</td>\n",
       "      <td>https://data.world/smartcolumbusos/650b7e59-af...</td>\n",
       "      <td>['csv']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['electric', 'vehicle', 'charging', 'stations']</td>\n",
       "      <td>['dataset', 'contains', 'session', 'details', ...</td>\n",
       "      <td>['dataset', 'contains', 'session', 'details', ...</td>\n",
       "      <td>['electric vehicle', 'environment', 'energy']</td>\n",
       "      <td>https://data.world/townofcary/electric-vehicle...</td>\n",
       "      <td>['dbf', 'csv', 'shx', 'shp', 'json', 'prj']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['nyserda', 'electric', 'vehicle', 'drive', 'c...</td>\n",
       "      <td>['new', 'york', 'state', '’', 'charge', 'ny', ...</td>\n",
       "      <td>['original', 'title', 'nyserda', 'electric', '...</td>\n",
       "      <td>['ev', 'electric vehicle', 'bev', 'phev', 'ghg...</td>\n",
       "      <td>https://data.world/data-ny-gov/thd2-fu8y</td>\n",
       "      <td>['csv']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['impact', 'uncoordinated', 'plugin', 'electri...</td>\n",
       "      <td>['impact', 'uncoordinated', 'plugin', 'electri...</td>\n",
       "      <td>['original', 'title', 'impact', 'uncoordinated...</td>\n",
       "      <td>['battery', 'consumption', 'data', 'energy', '...</td>\n",
       "      <td>https://data.world/us-doe-gov/8ae7e117-313b-40...</td>\n",
       "      <td>['xlsx']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  ['2023w7', 'global', 'electric', 'vehicle', 'm...   \n",
       "1  ['connected', 'electric', 'autonomous', 'vehic...   \n",
       "2    ['electric', 'vehicle', 'charging', 'stations']   \n",
       "3  ['nyserda', 'electric', 'vehicle', 'drive', 'c...   \n",
       "4  ['impact', 'uncoordinated', 'plugin', 'electri...   \n",
       "\n",
       "                                         description  \\\n",
       "0  ['global', 'electric', 'vehicle', 'market', 's...   \n",
       "1  ['connected', 'electric', 'autonomous', 'vehic...   \n",
       "2  ['dataset', 'contains', 'session', 'details', ...   \n",
       "3  ['new', 'york', 'state', '’', 'charge', 'ny', ...   \n",
       "4  ['impact', 'uncoordinated', 'plugin', 'electri...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  ['editorsimple', 'original', 'visualization', ...   \n",
       "1  ['rolling', 'snapshot', 'data', 'collected', '...   \n",
       "2  ['dataset', 'contains', 'session', 'details', ...   \n",
       "3  ['original', 'title', 'nyserda', 'electric', '...   \n",
       "4  ['original', 'title', 'impact', 'uncoordinated...   \n",
       "\n",
       "                                                tags  \\\n",
       "0  ['makeover monday', 'cars', 'vehicles', 'elect...   \n",
       "1                                     ['autonomous']   \n",
       "2      ['electric vehicle', 'environment', 'energy']   \n",
       "3  ['ev', 'electric vehicle', 'bev', 'phev', 'ghg...   \n",
       "4  ['battery', 'consumption', 'data', 'energy', '...   \n",
       "\n",
       "                                         dataset_url  \\\n",
       "0           https://data.world/makeovermonday/2023w7   \n",
       "1  https://data.world/smartcolumbusos/650b7e59-af...   \n",
       "2  https://data.world/townofcary/electric-vehicle...   \n",
       "3           https://data.world/data-ny-gov/thd2-fu8y   \n",
       "4  https://data.world/us-doe-gov/8ae7e117-313b-40...   \n",
       "\n",
       "                             available_formats  \n",
       "0                                     ['xlsx']  \n",
       "1                                      ['csv']  \n",
       "2  ['dbf', 'csv', 'shx', 'shp', 'json', 'prj']  \n",
       "3                                      ['csv']  \n",
       "4                                     ['xlsx']  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Metadata.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Values:\n",
      "title                0\n",
      "description          0\n",
      "summary              0\n",
      "tags                 0\n",
      "dataset_url          0\n",
      "available_formats    0\n",
      "dtype: int64\n",
      "\n",
      "NA Values:\n",
      "title                0\n",
      "description          0\n",
      "summary              0\n",
      "tags                 0\n",
      "dataset_url          0\n",
      "available_formats    0\n",
      "dtype: int64\n",
      "\n",
      "NaN Values:\n",
      "title                0\n",
      "description          0\n",
      "summary              0\n",
      "tags                 0\n",
      "dataset_url          0\n",
      "available_formats    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your dataset is loaded into a DataFrame called 'metadata_df'\n",
    "# Replace 'metadata_df' with the actual name of your DataFrame\n",
    "\n",
    "# Check for null values in each column\n",
    "null_values = Metadata.isnull().sum()\n",
    "print(\"Null Values:\")\n",
    "print(null_values)\n",
    "\n",
    "# Check for NA values in each column\n",
    "na_values = Metadata.isna().sum()\n",
    "print(\"\\nNA Values:\")\n",
    "print(na_values)\n",
    "\n",
    "# Check for NaN values in each column\n",
    "nan_values = Metadata.isna().sum()\n",
    "print(\"\\nNaN Values:\")\n",
    "print(nan_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 0.5495863080024719, Train Accuracy: 0.7761194029850746\n",
      "Validation Loss: 0.2011123076081276, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 2/10\n",
      "Train Loss: 0.12765191234648227, Train Accuracy: 0.9850746268656716\n",
      "Validation Loss: 0.013903766870498657, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 3/10\n",
      "Train Loss: 0.01584861446171999, Train Accuracy: 1.0\n",
      "Validation Loss: 0.004434770671650767, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 4/10\n",
      "Train Loss: 0.003796592028811574, Train Accuracy: 1.0\n",
      "Validation Loss: 0.001641700859181583, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 5/10\n",
      "Train Loss: 0.0016678876825608313, Train Accuracy: 1.0\n",
      "Validation Loss: 0.0004893661680398509, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 6/10\n",
      "Train Loss: 0.0006135981995612383, Train Accuracy: 1.0\n",
      "Validation Loss: 0.00024754069454502314, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 7/10\n",
      "Train Loss: 0.0003407398820854723, Train Accuracy: 1.0\n",
      "Validation Loss: 0.00017844399553723633, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 8/10\n",
      "Train Loss: 0.00032728462829254565, Train Accuracy: 1.0\n",
      "Validation Loss: 0.00014846034173388034, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 9/10\n",
      "Train Loss: 0.00024844754952937367, Train Accuracy: 1.0\n",
      "Validation Loss: 0.00012994451390113682, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 10/10\n",
      "Train Loss: 0.0002547768410295248, Train Accuracy: 1.0\n",
      "Validation Loss: 0.00011629674554569647, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Assuming MetaData_DS['description'] contains the dataset descriptions\n",
    "MetaData_DS = Metadata.copy()\n",
    "\n",
    "# Initialize DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Define maximum sequence length\n",
    "max_length = 128  # You can adjust this according to your dataset and memory constraints\n",
    "\n",
    "# Tokenize dataset descriptions\n",
    "tokenized_descriptions = tokenizer(list(MetaData_DS['description']), truncation=True, padding=True, max_length=max_length)\n",
    "\n",
    "# Convert labels to numerical format\n",
    "labels = [1 if url else 0 for url in MetaData_DS['dataset_url']]\n",
    "\n",
    "# Extract input_ids and attention_mask\n",
    "input_ids = tokenized_descriptions['input_ids']\n",
    "attention_mask = tokenized_descriptions['attention_mask']\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_ids, val_ids, train_masks, val_masks, train_labels, val_labels = train_test_split(input_ids, attention_mask, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert tokenized texts and labels to PyTorch tensors\n",
    "train_ids = torch.tensor(train_ids)\n",
    "val_ids = torch.tensor(val_ids)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "# Create TensorDataset for training and validation sets\n",
    "train_dataset = TensorDataset(train_ids, train_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_ids, val_masks, val_labels)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Create DataLoader for training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the DistilBERT-based model with increased dropout\n",
    "class DistilBERTClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(DistilBERTClassifier, self).__init__()\n",
    "        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.3)  # Dropout rate\n",
    "        self.linear = nn.Linear(self.distilbert.config.hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = outputs.last_hidden_state[:, 0]  # Take [CLS] token representation\n",
    "        hidden_state = self.dropout(hidden_state)\n",
    "        logits = self.linear(hidden_state)\n",
    "        return logits\n",
    "\n",
    "# Instantiate the DistilBERT model\n",
    "num_classes = 2  # Adjust according to your task\n",
    "model = DistilBERTClassifier(num_classes)\n",
    "\n",
    "# Define the optimizer with weight decay and gradient clipping\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=1, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop with early stopping\n",
    "epochs = 10\n",
    "patience = 2\n",
    "best_loss = float('inf')\n",
    "trigger_times = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for batch in train_loader:\n",
    "        inputs, attention_masks, labels = batch\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs, attention_masks)\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy for this batch\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = total_correct / total_samples\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, attention_masks, labels = batch\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs, attention_masks)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Compute predictions\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "            # Update total samples and total correct predictions\n",
    "            total_samples += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Save predictions and labels for metric calculation\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='binary')\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    print(f\"Train Loss: {avg_train_loss}, Train Accuracy: {train_accuracy}\")\n",
    "    print(f\"Validation Loss: {avg_val_loss}, Validation Accuracy: {accuracy}\")\n",
    "    print(f\"Validation Precision: {precision}, Validation Recall: {recall}, Validation F1 Score: {f1}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        trigger_times = 0\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print('Early stopping!')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 0.5273704051971435, Train Accuracy: 0.6865671641791045\n",
      "Validation Loss: 0.11749618873000145, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 2/10\n",
      "Train Loss: 0.06711598448455333, Train Accuracy: 1.0\n",
      "Validation Loss: 0.005186932859942317, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 3/10\n",
      "Train Loss: 0.005143722286447883, Train Accuracy: 1.0\n",
      "Validation Loss: 0.0016466716770082712, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 4/10\n",
      "Train Loss: 0.0019688812317326665, Train Accuracy: 1.0\n",
      "Validation Loss: 0.000774408457800746, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 5/10\n",
      "Train Loss: 0.0008373793913051486, Train Accuracy: 1.0\n",
      "Validation Loss: 0.00037346838507801294, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 6/10\n",
      "Train Loss: 0.0005050423264037817, Train Accuracy: 1.0\n",
      "Validation Loss: 0.00021841219131601974, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 7/10\n",
      "Train Loss: 0.00030835767393000424, Train Accuracy: 1.0\n",
      "Validation Loss: 0.0001542672616778873, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 8/10\n",
      "Train Loss: 0.00023023397370707243, Train Accuracy: 1.0\n",
      "Validation Loss: 0.00012364218127913773, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 9/10\n",
      "Train Loss: 0.00019912350981030614, Train Accuracy: 1.0\n",
      "Validation Loss: 0.00010681698768166825, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 10/10\n",
      "Train Loss: 0.0002030141680734232, Train Accuracy: 1.0\n",
      "Validation Loss: 9.599611803423613e-05, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Assuming MetaData_DS['description'] contains the dataset descriptions\n",
    "MetaData_DS = Metadata.copy()\n",
    "\n",
    "# Initialize DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Define maximum sequence length\n",
    "max_length = 128  # You can adjust this according to your dataset and memory constraints\n",
    "\n",
    "# Tokenize dataset descriptions\n",
    "tokenized_descriptions = tokenizer(list(MetaData_DS['description']), truncation=True, padding=True, max_length=max_length)\n",
    "\n",
    "# Convert labels to numerical format\n",
    "labels = [1 if url else 0 for url in MetaData_DS['dataset_url']]\n",
    "\n",
    "# Extract input_ids and attention_mask\n",
    "input_ids = tokenized_descriptions['input_ids']\n",
    "attention_mask = tokenized_descriptions['attention_mask']\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_ids, val_ids, train_masks, val_masks, train_labels, val_labels = train_test_split(input_ids, attention_mask, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert tokenized texts and labels to PyTorch tensors\n",
    "train_ids = torch.tensor(train_ids)\n",
    "val_ids = torch.tensor(val_ids)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "# Create TensorDataset for training and validation sets\n",
    "train_dataset = TensorDataset(train_ids, train_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_ids, val_masks, val_labels)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Create DataLoader for training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the DistilBERT-based model with increased dropout and weight decay\n",
    "class DistilBERTClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(DistilBERTClassifier, self).__init__()\n",
    "        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.5)  # Increased dropout rate\n",
    "        self.linear = nn.Linear(self.distilbert.config.hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = outputs.last_hidden_state[:, 0]  # Take [CLS] token representation\n",
    "        hidden_state = self.dropout(hidden_state)\n",
    "        logits = self.linear(hidden_state)\n",
    "        return logits\n",
    "\n",
    "# Instantiate the DistilBERT model\n",
    "num_classes = 2  # Adjust according to your task\n",
    "model = DistilBERTClassifier(num_classes)\n",
    "\n",
    "# Define the optimizer with weight decay and gradient clipping\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)  # Increased weight decay\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=1, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop with early stopping and gradient clipping\n",
    "epochs = 10\n",
    "patience = 2\n",
    "best_loss = float('inf')\n",
    "trigger_times = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for batch in train_loader:\n",
    "        inputs, attention_masks, labels = batch\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs, attention_masks)\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy for this batch\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = total_correct / total_samples\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, attention_masks, labels = batch\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs, attention_masks)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Compute predictions\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "            # Update total samples and total correct predictions\n",
    "            total_samples += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Save predictions and labels for metric calculation\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='binary')\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    print(f\"Train Loss: {avg_train_loss}, Train Accuracy: {train_accuracy}\")\n",
    "    print(f\"Validation Loss: {avg_val_loss}, Validation Accuracy: {accuracy}\")\n",
    "    print(f\"Validation Precision: {precision}, Validation Recall: {recall}, Validation F1 Score: {f1}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        trigger_times = 0\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print('Early stopping!')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ['connected', 'electric', 'autonomous', 'vehicle']\n",
      "Dataset URL: https://data.world/smartcolumbusos/650b7e59-afd3-4c40-9bfc-8d614610b77b\n",
      "Cosine Similarity: [[0.3919043]]\n",
      "\n",
      "Title: ['energy', 'star', 'certified', 'electric', 'vehicle', 'supply', 'equipment']\n",
      "Dataset URL: https://data.world/us-epa-gov/f3822503-5d5e-4cac-ab1c-c8c18847bc7e\n",
      "Cosine Similarity: [[0.3810537]]\n",
      "\n",
      "Title: ['electric', 'vehicles', 'charging', 'points']\n",
      "Dataset URL: https://data.world/datagov-uk/e865fb83-66fb-4b5e-a95e-cfbb68a7beb5\n",
      "Cosine Similarity: [[0.41192517]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load DistilBERT tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Assuming MetaData_DS['description'] contains the dataset descriptions\n",
    "MetaData_DS = Metadata.copy()\n",
    "\n",
    "# Tokenize dataset descriptions\n",
    "tokenized_descriptions = tokenizer(list(MetaData_DS['description']), truncation=True, padding=True, max_length=max_length)\n",
    "\n",
    "# Convert tokenized texts to PyTorch tensors\n",
    "dataset_ids = torch.tensor(tokenized_descriptions['input_ids'])\n",
    "dataset_masks = torch.tensor(tokenized_descriptions['attention_mask'])\n",
    "\n",
    "# Create DataLoader for dataset descriptions\n",
    "dataset_loader = DataLoader(TensorDataset(dataset_ids, dataset_masks), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Get DistilBERT embeddings for dataset descriptions\n",
    "dataset_descriptions = []\n",
    "with torch.no_grad():\n",
    "    for batch in dataset_loader:\n",
    "        inputs, attention_masks = batch\n",
    "        outputs = model(inputs, attention_masks)\n",
    "        embeddings = torch.mean(outputs.last_hidden_state, dim=1)  # Mean pooling of token embeddings\n",
    "        dataset_descriptions.append(embeddings.numpy())\n",
    "\n",
    "# Convert to numpy array\n",
    "dataset_descriptions = np.concatenate(dataset_descriptions, axis=0)\n",
    "\n",
    "# Function to compute DistilBERT embeddings\n",
    "def get_distilbert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=128, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = torch.mean(outputs.last_hidden_state, dim=1)  # Mean pooling of token embeddings\n",
    "    return embeddings\n",
    "\n",
    "# Function to predict dataset URLs based on search text\n",
    "def predict_dataset_url(search_text, dataset_descriptions):\n",
    "    search_emb = get_distilbert_embedding(search_text)\n",
    "    \n",
    "    # Reshape the search_emb array to 2D\n",
    "    search_emb = search_emb.reshape(1, -1)\n",
    "    \n",
    "    similarities = [cosine_similarity(search_emb, dataset_emb.reshape(1, -1)) for dataset_emb in dataset_descriptions]\n",
    "    \n",
    "    # Find the maximum and minimum similarity values\n",
    "    max_similarity = max(similarities)\n",
    "    min_similarity = min(similarities)\n",
    "    \n",
    "    # Calculate the 90% range\n",
    "    range_90 = 0.80 * (max_similarity - min_similarity)\n",
    "    \n",
    "    # Find the threshold value\n",
    "    threshold = min_similarity + range_90\n",
    "    \n",
    "    # Find the indices of datasets with similarity above the threshold\n",
    "    similar_indices = [i for i, sim in enumerate(similarities) if sim > threshold]\n",
    "    \n",
    "    results = []\n",
    "    for idx in similar_indices:\n",
    "        dataset_info = {\n",
    "            \"title\": MetaData_DS.iloc[idx]['title'],\n",
    "            \"dataset_url\": MetaData_DS.iloc[idx]['dataset_url'],\n",
    "            \"cosine_similarity\": similarities[idx]\n",
    "        }\n",
    "        results.append(dataset_info)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "search_text = \"electric vehicles\"\n",
    "results = predict_dataset_url(search_text, dataset_descriptions)\n",
    "\n",
    "# Display results\n",
    "if not results:\n",
    "    print(\"No datasets found above the 90% threshold.\")\n",
    "else:\n",
    "    for result in results:\n",
    "        print(f\"Title: {result['title']}\")\n",
    "        print(f\"Dataset URL: {result['dataset_url']}\")\n",
    "        print(f\"Cosine Similarity: {result['cosine_similarity']}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>summary</th>\n",
       "      <th>tags</th>\n",
       "      <th>dataset_url</th>\n",
       "      <th>available_formats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['2023w7', 'global', 'electric', 'vehicle', 'm...</td>\n",
       "      <td>['global', 'electric', 'vehicle', 'market', 's...</td>\n",
       "      <td>['editorsimple', 'original', 'visualization', ...</td>\n",
       "      <td>['makeover monday', 'cars', 'vehicles', 'elect...</td>\n",
       "      <td>https://data.world/makeovermonday/2023w7</td>\n",
       "      <td>['xlsx']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['connected', 'electric', 'autonomous', 'vehic...</td>\n",
       "      <td>['connected', 'electric', 'autonomous', 'vehic...</td>\n",
       "      <td>['rolling', 'snapshot', 'data', 'collected', '...</td>\n",
       "      <td>['autonomous']</td>\n",
       "      <td>https://data.world/smartcolumbusos/650b7e59-af...</td>\n",
       "      <td>['csv']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['electric', 'vehicle', 'charging', 'stations']</td>\n",
       "      <td>['dataset', 'contains', 'session', 'details', ...</td>\n",
       "      <td>['dataset', 'contains', 'session', 'details', ...</td>\n",
       "      <td>['electric vehicle', 'environment', 'energy']</td>\n",
       "      <td>https://data.world/townofcary/electric-vehicle...</td>\n",
       "      <td>['dbf', 'csv', 'shx', 'shp', 'json', 'prj']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['nyserda', 'electric', 'vehicle', 'drive', 'c...</td>\n",
       "      <td>['new', 'york', 'state', '’', 'charge', 'ny', ...</td>\n",
       "      <td>['original', 'title', 'nyserda', 'electric', '...</td>\n",
       "      <td>['ev', 'electric vehicle', 'bev', 'phev', 'ghg...</td>\n",
       "      <td>https://data.world/data-ny-gov/thd2-fu8y</td>\n",
       "      <td>['csv']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['impact', 'uncoordinated', 'plugin', 'electri...</td>\n",
       "      <td>['impact', 'uncoordinated', 'plugin', 'electri...</td>\n",
       "      <td>['original', 'title', 'impact', 'uncoordinated...</td>\n",
       "      <td>['battery', 'consumption', 'data', 'energy', '...</td>\n",
       "      <td>https://data.world/us-doe-gov/8ae7e117-313b-40...</td>\n",
       "      <td>['xlsx']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  ['2023w7', 'global', 'electric', 'vehicle', 'm...   \n",
       "1  ['connected', 'electric', 'autonomous', 'vehic...   \n",
       "2    ['electric', 'vehicle', 'charging', 'stations']   \n",
       "3  ['nyserda', 'electric', 'vehicle', 'drive', 'c...   \n",
       "4  ['impact', 'uncoordinated', 'plugin', 'electri...   \n",
       "\n",
       "                                         description  \\\n",
       "0  ['global', 'electric', 'vehicle', 'market', 's...   \n",
       "1  ['connected', 'electric', 'autonomous', 'vehic...   \n",
       "2  ['dataset', 'contains', 'session', 'details', ...   \n",
       "3  ['new', 'york', 'state', '’', 'charge', 'ny', ...   \n",
       "4  ['impact', 'uncoordinated', 'plugin', 'electri...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  ['editorsimple', 'original', 'visualization', ...   \n",
       "1  ['rolling', 'snapshot', 'data', 'collected', '...   \n",
       "2  ['dataset', 'contains', 'session', 'details', ...   \n",
       "3  ['original', 'title', 'nyserda', 'electric', '...   \n",
       "4  ['original', 'title', 'impact', 'uncoordinated...   \n",
       "\n",
       "                                                tags  \\\n",
       "0  ['makeover monday', 'cars', 'vehicles', 'elect...   \n",
       "1                                     ['autonomous']   \n",
       "2      ['electric vehicle', 'environment', 'energy']   \n",
       "3  ['ev', 'electric vehicle', 'bev', 'phev', 'ghg...   \n",
       "4  ['battery', 'consumption', 'data', 'energy', '...   \n",
       "\n",
       "                                         dataset_url  \\\n",
       "0           https://data.world/makeovermonday/2023w7   \n",
       "1  https://data.world/smartcolumbusos/650b7e59-af...   \n",
       "2  https://data.world/townofcary/electric-vehicle...   \n",
       "3           https://data.world/data-ny-gov/thd2-fu8y   \n",
       "4  https://data.world/us-doe-gov/8ae7e117-313b-40...   \n",
       "\n",
       "                             available_formats  \n",
       "0                                     ['xlsx']  \n",
       "1                                      ['csv']  \n",
       "2  ['dbf', 'csv', 'shx', 'shp', 'json', 'prj']  \n",
       "3                                      ['csv']  \n",
       "4                                     ['xlsx']  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MetaData_DS=Metadata.copy()\n",
    "MetaData_DS.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "MetaData_DS['text'] = MetaData_DS['title'] + \" \" + MetaData_DS['description'] + \" \" + MetaData_DS['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>summary</th>\n",
       "      <th>tags</th>\n",
       "      <th>dataset_url</th>\n",
       "      <th>available_formats</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['2023w7', 'global', 'electric', 'vehicle', 'm...</td>\n",
       "      <td>['global', 'electric', 'vehicle', 'market', 's...</td>\n",
       "      <td>['editorsimple', 'original', 'visualization', ...</td>\n",
       "      <td>['makeover monday', 'cars', 'vehicles', 'elect...</td>\n",
       "      <td>https://data.world/makeovermonday/2023w7</td>\n",
       "      <td>['xlsx']</td>\n",
       "      <td>['2023w7', 'global', 'electric', 'vehicle', 'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['connected', 'electric', 'autonomous', 'vehic...</td>\n",
       "      <td>['connected', 'electric', 'autonomous', 'vehic...</td>\n",
       "      <td>['rolling', 'snapshot', 'data', 'collected', '...</td>\n",
       "      <td>['autonomous']</td>\n",
       "      <td>https://data.world/smartcolumbusos/650b7e59-af...</td>\n",
       "      <td>['csv']</td>\n",
       "      <td>['connected', 'electric', 'autonomous', 'vehic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['electric', 'vehicle', 'charging', 'stations']</td>\n",
       "      <td>['dataset', 'contains', 'session', 'details', ...</td>\n",
       "      <td>['dataset', 'contains', 'session', 'details', ...</td>\n",
       "      <td>['electric vehicle', 'environment', 'energy']</td>\n",
       "      <td>https://data.world/townofcary/electric-vehicle...</td>\n",
       "      <td>['dbf', 'csv', 'shx', 'shp', 'json', 'prj']</td>\n",
       "      <td>['electric', 'vehicle', 'charging', 'stations'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['nyserda', 'electric', 'vehicle', 'drive', 'c...</td>\n",
       "      <td>['new', 'york', 'state', '’', 'charge', 'ny', ...</td>\n",
       "      <td>['original', 'title', 'nyserda', 'electric', '...</td>\n",
       "      <td>['ev', 'electric vehicle', 'bev', 'phev', 'ghg...</td>\n",
       "      <td>https://data.world/data-ny-gov/thd2-fu8y</td>\n",
       "      <td>['csv']</td>\n",
       "      <td>['nyserda', 'electric', 'vehicle', 'drive', 'c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['impact', 'uncoordinated', 'plugin', 'electri...</td>\n",
       "      <td>['impact', 'uncoordinated', 'plugin', 'electri...</td>\n",
       "      <td>['original', 'title', 'impact', 'uncoordinated...</td>\n",
       "      <td>['battery', 'consumption', 'data', 'energy', '...</td>\n",
       "      <td>https://data.world/us-doe-gov/8ae7e117-313b-40...</td>\n",
       "      <td>['xlsx']</td>\n",
       "      <td>['impact', 'uncoordinated', 'plugin', 'electri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  ['2023w7', 'global', 'electric', 'vehicle', 'm...   \n",
       "1  ['connected', 'electric', 'autonomous', 'vehic...   \n",
       "2    ['electric', 'vehicle', 'charging', 'stations']   \n",
       "3  ['nyserda', 'electric', 'vehicle', 'drive', 'c...   \n",
       "4  ['impact', 'uncoordinated', 'plugin', 'electri...   \n",
       "\n",
       "                                         description  \\\n",
       "0  ['global', 'electric', 'vehicle', 'market', 's...   \n",
       "1  ['connected', 'electric', 'autonomous', 'vehic...   \n",
       "2  ['dataset', 'contains', 'session', 'details', ...   \n",
       "3  ['new', 'york', 'state', '’', 'charge', 'ny', ...   \n",
       "4  ['impact', 'uncoordinated', 'plugin', 'electri...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  ['editorsimple', 'original', 'visualization', ...   \n",
       "1  ['rolling', 'snapshot', 'data', 'collected', '...   \n",
       "2  ['dataset', 'contains', 'session', 'details', ...   \n",
       "3  ['original', 'title', 'nyserda', 'electric', '...   \n",
       "4  ['original', 'title', 'impact', 'uncoordinated...   \n",
       "\n",
       "                                                tags  \\\n",
       "0  ['makeover monday', 'cars', 'vehicles', 'elect...   \n",
       "1                                     ['autonomous']   \n",
       "2      ['electric vehicle', 'environment', 'energy']   \n",
       "3  ['ev', 'electric vehicle', 'bev', 'phev', 'ghg...   \n",
       "4  ['battery', 'consumption', 'data', 'energy', '...   \n",
       "\n",
       "                                         dataset_url  \\\n",
       "0           https://data.world/makeovermonday/2023w7   \n",
       "1  https://data.world/smartcolumbusos/650b7e59-af...   \n",
       "2  https://data.world/townofcary/electric-vehicle...   \n",
       "3           https://data.world/data-ny-gov/thd2-fu8y   \n",
       "4  https://data.world/us-doe-gov/8ae7e117-313b-40...   \n",
       "\n",
       "                             available_formats  \\\n",
       "0                                     ['xlsx']   \n",
       "1                                      ['csv']   \n",
       "2  ['dbf', 'csv', 'shx', 'shp', 'json', 'prj']   \n",
       "3                                      ['csv']   \n",
       "4                                     ['xlsx']   \n",
       "\n",
       "                                                text  \n",
       "0  ['2023w7', 'global', 'electric', 'vehicle', 'm...  \n",
       "1  ['connected', 'electric', 'autonomous', 'vehic...  \n",
       "2  ['electric', 'vehicle', 'charging', 'stations'...  \n",
       "3  ['nyserda', 'electric', 'vehicle', 'drive', 'c...  \n",
       "4  ['impact', 'uncoordinated', 'plugin', 'electri...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MetaData_DS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 0.4420580014586449, Train Accuracy: 0.7910447761194029\n",
      "Validation Loss: 0.074073800817132, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 2/10\n",
      "Train Loss: 0.032543318904936315, Train Accuracy: 1.0\n",
      "Validation Loss: 0.006084063556045294, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 3/10\n",
      "Train Loss: 0.004791516880504787, Train Accuracy: 1.0\n",
      "Validation Loss: 0.0018775270436890423, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 4/10\n",
      "Train Loss: 0.0013390567270107566, Train Accuracy: 1.0\n",
      "Validation Loss: 0.000665990897687152, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 5/10\n",
      "Train Loss: 0.0006014390208292752, Train Accuracy: 1.0\n",
      "Validation Loss: 0.0003541438636602834, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 6/10\n",
      "Train Loss: 0.0003374495485331863, Train Accuracy: 1.0\n",
      "Validation Loss: 0.0002518124820198864, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 7/10\n",
      "Train Loss: 0.0002811410464346409, Train Accuracy: 1.0\n",
      "Validation Loss: 0.00020548442989820614, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 8/10\n",
      "Train Loss: 0.00024155903374776245, Train Accuracy: 1.0\n",
      "Validation Loss: 0.0001783770276233554, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 9/10\n",
      "Train Loss: 0.00020525574218481778, Train Accuracy: 1.0\n",
      "Validation Loss: 0.00015935891133267432, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 10/10\n",
      "Train Loss: 0.0002046667825197801, Train Accuracy: 1.0\n",
      "Validation Loss: 0.00014551783533534035, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "\n",
    "# Initialize DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Define maximum sequence length\n",
    "max_length = 128  # You can adjust this according to your dataset and memory constraints\n",
    "\n",
    "# Tokenize dataset descriptions\n",
    "tokenized_descriptions = tokenizer(list(MetaData_DS['text']), truncation=True, padding=True, max_length=max_length)\n",
    "\n",
    "# Convert labels to numerical format\n",
    "labels = [1 if url else 0 for url in MetaData_DS['dataset_url']]\n",
    "\n",
    "# Extract input_ids and attention_mask\n",
    "input_ids = tokenized_descriptions['input_ids']\n",
    "attention_mask = tokenized_descriptions['attention_mask']\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_ids, val_ids, train_masks, val_masks, train_labels, val_labels = train_test_split(input_ids, attention_mask, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert tokenized texts and labels to PyTorch tensors\n",
    "train_ids = torch.tensor(train_ids)\n",
    "val_ids = torch.tensor(val_ids)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "# Create TensorDataset for training and validation sets\n",
    "train_dataset = TensorDataset(train_ids, train_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_ids, val_masks, val_labels)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Create DataLoader for training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the DistilBERT-based model with increased dropout and weight decay\n",
    "class DistilBERTClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(DistilBERTClassifier, self).__init__()\n",
    "        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.5)  # Increased dropout rate\n",
    "        self.linear = nn.Linear(self.distilbert.config.hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = outputs.last_hidden_state[:, 0]  # Take [CLS] token representation\n",
    "        hidden_state = self.dropout(hidden_state)\n",
    "        logits = self.linear(hidden_state)\n",
    "        return logits\n",
    "\n",
    "# Instantiate the DistilBERT model\n",
    "num_classes = 2  # Adjust according to your task\n",
    "model = DistilBERTClassifier(num_classes)\n",
    "\n",
    "# Define the optimizer with weight decay and gradient clipping\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)  # Increased weight decay\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=1, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop with early stopping and gradient clipping\n",
    "epochs = 10\n",
    "patience = 2\n",
    "best_loss = float('inf')\n",
    "trigger_times = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for batch in train_loader:\n",
    "        inputs, attention_masks, labels = batch\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs, attention_masks)\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy for this batch\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = total_correct / total_samples\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, attention_masks, labels = batch\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs, attention_masks)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Compute predictions\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "            # Update total samples and total correct predictions\n",
    "            total_samples += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Save predictions and labels for metric calculation\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='binary')\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    print(f\"Train Loss: {avg_train_loss}, Train Accuracy: {train_accuracy}\")\n",
    "    print(f\"Validation Loss: {avg_val_loss}, Validation Accuracy: {accuracy}\")\n",
    "    print(f\"Validation Precision: {precision}, Validation Recall: {recall}, Validation F1 Score: {f1}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        trigger_times = 0\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print('Early stopping!')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ['electric', 'vehicle', 'year']\n",
      "Dataset URL: https://data.world/jeffgswanson/electric-vehicle-by-year\n",
      "Cosine Similarity: [[0.3201803]]\n",
      "\n",
      "Title: ['nys', 'electric', 'vehicles', 'data']\n",
      "Dataset URL: https://data.world/john-rager/nys-electric-vehicles-data\n",
      "Cosine Similarity: [[0.33808926]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load DistilBERT tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize dataset descriptions\n",
    "tokenized_descriptions = tokenizer(list(MetaData_DS['text']), truncation=True, padding=True, max_length=max_length)\n",
    "\n",
    "# Convert tokenized texts to PyTorch tensors\n",
    "dataset_ids = torch.tensor(tokenized_descriptions['input_ids'])\n",
    "dataset_masks = torch.tensor(tokenized_descriptions['attention_mask'])\n",
    "\n",
    "# Create DataLoader for dataset descriptions\n",
    "dataset_loader = DataLoader(TensorDataset(dataset_ids, dataset_masks), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Get DistilBERT embeddings for dataset descriptions\n",
    "dataset_descriptions = []\n",
    "with torch.no_grad():\n",
    "    for batch in dataset_loader:\n",
    "        inputs, attention_masks = batch\n",
    "        outputs = model(inputs, attention_masks)\n",
    "        embeddings = torch.mean(outputs.last_hidden_state, dim=1)  # Mean pooling of token embeddings\n",
    "        dataset_descriptions.append(embeddings.numpy())\n",
    "\n",
    "# Convert to numpy array\n",
    "dataset_descriptions = np.concatenate(dataset_descriptions, axis=0)\n",
    "\n",
    "# Function to compute DistilBERT embeddings\n",
    "def get_distilbert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=128, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = torch.mean(outputs.last_hidden_state, dim=1)  # Mean pooling of token embeddings\n",
    "    return embeddings\n",
    "\n",
    "# Function to predict dataset URLs based on search text\n",
    "def predict_dataset_url(search_text, dataset_descriptions):\n",
    "    search_emb = get_distilbert_embedding(search_text)\n",
    "    \n",
    "    # Reshape the search_emb array to 2D\n",
    "    search_emb = search_emb.reshape(1, -1)\n",
    "    \n",
    "    similarities = [cosine_similarity(search_emb, dataset_emb.reshape(1, -1)) for dataset_emb in dataset_descriptions]\n",
    "    \n",
    "    # Find the maximum and minimum similarity values\n",
    "    max_similarity = max(similarities)\n",
    "    min_similarity = min(similarities)\n",
    "    \n",
    "    # Calculate the 90% range\n",
    "    range_90 = 0.80 * (max_similarity - min_similarity)\n",
    "    \n",
    "    # Find the threshold value\n",
    "    threshold = min_similarity + range_90\n",
    "    \n",
    "    # Find the indices of datasets with similarity above the threshold\n",
    "    similar_indices = [i for i, sim in enumerate(similarities) if sim > threshold]\n",
    "    \n",
    "    results = []\n",
    "    for idx in similar_indices:\n",
    "        dataset_info = {\n",
    "            \"title\": MetaData_DS.iloc[idx]['title'],\n",
    "            \"dataset_url\": MetaData_DS.iloc[idx]['dataset_url'],\n",
    "            \"cosine_similarity\": similarities[idx]\n",
    "        }\n",
    "        results.append(dataset_info)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "search_text = \"electric vehicles\"\n",
    "results = predict_dataset_url(search_text, dataset_descriptions)\n",
    "\n",
    "# Display results\n",
    "if not results:\n",
    "    print(\"No datasets found above the 90% threshold.\")\n",
    "else:\n",
    "    for result in results:\n",
    "        print(f\"Title: {result['title']}\")\n",
    "        print(f\"Dataset URL: {result['dataset_url']}\")\n",
    "        print(f\"Cosine Similarity: {result['cosine_similarity']}\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
