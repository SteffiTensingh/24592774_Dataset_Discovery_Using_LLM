{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages Installation\n",
    "import pandas as pd\n",
    "from transformers import Trainer, TrainingArguments, T5ForConditionalGeneration, T5Tokenizer\n",
    "from datasets import load_metric\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Metadata = pd.read_csv('C:\\\\24592774_Dataset_Discovery_Using_LLM\\\\MetaData_Creation\\\\MetaData_Notebooks\\\\Prepared_MetaData_DataSet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Metadata_DS = Metadata.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Metadata_DS['text'] = Metadata_DS['title']+Metadata_DS['description']+Metadata_DS['summary']+Metadata_DS['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Temp\\ipykernel_21492\\2262684198.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_ids = torch.tensor(train_ids)\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Temp\\ipykernel_21492\\2262684198.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_ids = torch.tensor(val_ids)\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Temp\\ipykernel_21492\\2262684198.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_masks = torch.tensor(train_masks)\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Temp\\ipykernel_21492\\2262684198.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_masks = torch.tensor(val_masks)\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 0.24539896696805955, Train Accuracy: 0.9701492537313433\n",
      "Validation Loss: 0.057477476075291634, Validation Accuracy: 0.5\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 2/10\n",
      "Train Loss: 0.04670276567339897, Train Accuracy: 1.0\n",
      "Validation Loss: 0.010904028546065092, Validation Accuracy: 0.5\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 3/10\n",
      "Train Loss: 0.018700627610087395, Train Accuracy: 1.0\n",
      "Validation Loss: 0.0038369958056136966, Validation Accuracy: 0.5\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 4/10\n",
      "Train Loss: 0.014899472892284393, Train Accuracy: 1.0\n",
      "Validation Loss: 0.0019913959549739957, Validation Accuracy: 0.5\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 5/10\n",
      "Train Loss: 0.003919217688962817, Train Accuracy: 1.0\n",
      "Validation Loss: 0.0011060097021982074, Validation Accuracy: 0.5\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 6/10\n",
      "Train Loss: 0.012013497343286873, Train Accuracy: 1.0\n",
      "Validation Loss: 0.000686442363075912, Validation Accuracy: 0.5\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 7/10\n",
      "Train Loss: 0.0020080549409613015, Train Accuracy: 1.0\n",
      "Validation Loss: 0.0004970159789081663, Validation Accuracy: 0.5\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 8/10\n",
      "Train Loss: 0.0012133329990319907, Train Accuracy: 1.0\n",
      "Validation Loss: 0.0003952271363232285, Validation Accuracy: 0.5\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 9/10\n",
      "Train Loss: 0.001677460502833128, Train Accuracy: 1.0\n",
      "Validation Loss: 0.0003293060144642368, Validation Accuracy: 0.5\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 10/10\n",
      "Train Loss: 0.0009288624976761639, Train Accuracy: 1.0\n",
      "Validation Loss: 0.00028191995806992054, Validation Accuracy: 0.5\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import ElectraTokenizer, ElectraModel, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Assuming MetaData_DS['description'] contains the dataset descriptions\n",
    "MetaData_DS = Metadata.copy()\n",
    "\n",
    "# Initialize ELECTRA tokenizer\n",
    "tokenizer = ElectraTokenizer.from_pretrained('google/electra-base-discriminator')\n",
    "\n",
    "# Define maximum sequence length\n",
    "max_length = 128  # You can adjust this according to your dataset and memory constraints\n",
    "\n",
    "# Tokenize dataset descriptions\n",
    "tokenized_descriptions = tokenizer(list(MetaData_DS['description']), truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
    "\n",
    "# Convert labels to numerical format\n",
    "labels = [1 if url else 0 for url in MetaData_DS['dataset_url']]\n",
    "\n",
    "# Extract input_ids and attention_mask\n",
    "input_ids = tokenized_descriptions['input_ids']\n",
    "attention_mask = tokenized_descriptions['attention_mask']\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_ids, val_ids, train_masks, val_masks, train_labels, val_labels = train_test_split(input_ids, attention_mask, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert tokenized texts and labels to PyTorch tensors\n",
    "train_ids = torch.tensor(train_ids)\n",
    "val_ids = torch.tensor(val_ids)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "# Create TensorDataset for training and validation sets\n",
    "train_dataset = TensorDataset(train_ids, train_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_ids, val_masks, val_labels)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Create DataLoader for training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the ELECTRA-based model with increased dropout\n",
    "class ElectraClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ElectraClassifier, self).__init__()\n",
    "        self.electra = ElectraModel.from_pretrained('google/electra-base-discriminator')\n",
    "        self.dropout = nn.Dropout(0.3)  # Dropout rate\n",
    "        self.linear = nn.Linear(self.electra.config.hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.electra(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = outputs.last_hidden_state[:, 0]  # Take [CLS] token representation\n",
    "        hidden_state = self.dropout(hidden_state)\n",
    "        logits = self.linear(hidden_state)\n",
    "        return logits\n",
    "\n",
    "# Instantiate the ELECTRA model\n",
    "num_classes = 2  # Adjust according to your task\n",
    "model = ElectraClassifier(num_classes)\n",
    "\n",
    "# Define the optimizer with weight decay and gradient clipping\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=1, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop with early stopping\n",
    "epochs = 10\n",
    "patience = 2\n",
    "best_loss = float('inf')\n",
    "trigger_times = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for batch in train_loader:\n",
    "        inputs, attention_masks, labels = batch\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs, attention_masks)\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy for this batch\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = total_correct / total_samples\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, attention_masks, labels = batch\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs, attention_masks)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Compute predictions\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "            # Update total samples and total correct predictions\n",
    "            total_samples += labels.size(0)\n",
    "                        # Update total samples and total correct predictions\n",
    "            total_samples += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Save predictions and labels for metric calculation\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='binary')\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    print(f\"Train Loss: {avg_train_loss}, Train Accuracy: {train_accuracy}\")\n",
    "    print(f\"Validation Loss: {avg_val_loss}, Validation Accuracy: {accuracy}\")\n",
    "    print(f\"Validation Precision: {precision}, Validation Recall: {recall}, Validation F1 Score: {f1}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        trigger_times = 0\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print('Early stopping!')\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ['json', 'repository']\n",
      "Dataset URL: https://data.world/hdx/e66dbc70-17fe-4230-b9d6-855d192fc05c\n",
      "Cosine Similarity: 0.7494983673095703\n",
      "\n",
      "Title: ['houseboats']\n",
      "Dataset URL: https://data.world/datagov-uk/0cd0d5c0-f170-4899-ba45-e7d227bbd0e4\n",
      "Cosine Similarity: 0.7750325798988342\n",
      "\n",
      "Title: ['thurrock', 'outdoor', 'sports']\n",
      "Dataset URL: https://data.world/datagov-uk/17c44e3a-804c-487b-a07f-b90298685e2a\n",
      "Cosine Similarity: 0.7471963167190552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraTokenizer, ElectraModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load ELECTRA tokenizer and model\n",
    "tokenizer = ElectraTokenizer.from_pretrained('google/electra-base-discriminator')\n",
    "model = ElectraModel.from_pretrained('google/electra-base-discriminator')\n",
    "model.eval()\n",
    "\n",
    "# Function to get ELECTRA embedding\n",
    "def get_electra_embedding(text, model, tokenizer, max_length=128):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=max_length)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use the last hidden state of the [CLS] token as the embedding\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "\n",
    "# Assuming MetaData_DS['description'] contains the dataset descriptions\n",
    "dataset_descriptions = Metadata['description']\n",
    "\n",
    "# Generate embeddings for all dataset descriptions\n",
    "dataset_embeddings = []\n",
    "for description in dataset_descriptions:\n",
    "    emb = get_electra_embedding(description, model, tokenizer)\n",
    "    dataset_embeddings.append(emb)\n",
    "\n",
    "# Function to predict dataset URLs based on search text\n",
    "def predict_dataset_url(search_text):\n",
    "    search_emb = get_electra_embedding(search_text, model, tokenizer)\n",
    "    similarities = [cosine_similarity([search_emb], [dataset_emb])[0][0] for dataset_emb in dataset_embeddings]\n",
    "    \n",
    "    # Find the maximum and minimum similarity values\n",
    "    max_similarity = max(similarities)\n",
    "    min_similarity = min(similarities)\n",
    "    \n",
    "    # Calculate the 90% range\n",
    "    range_90 = 0.90 * (max_similarity - min_similarity)\n",
    "    \n",
    "    # Find the threshold value\n",
    "    threshold = min_similarity + range_90\n",
    "    \n",
    "    # Find the indices of datasets with similarity above the threshold\n",
    "    similar_indices = [i for i, sim in enumerate(similarities) if sim > threshold]\n",
    "    \n",
    "    results = []\n",
    "    for idx in similar_indices:\n",
    "        dataset_info = {\n",
    "            \"title\": Metadata.iloc[idx]['title'],\n",
    "            \"dataset_url\": Metadata.iloc[idx]['dataset_url'],\n",
    "            \"cosine_similarity\": similarities[idx]\n",
    "        }\n",
    "        results.append(dataset_info)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "search_text = \"electric vehicles\"\n",
    "results = predict_dataset_url(search_text)\n",
    "\n",
    "# Display results\n",
    "if not results:\n",
    "    print(\"No datasets found above the 90% threshold.\")\n",
    "else:\n",
    "    for result in results:\n",
    "        print(f\"Title: {result['title']}\")\n",
    "        print(f\"Dataset URL: {result['dataset_url']}\")\n",
    "        print(f\"Cosine Similarity: {result['cosine_similarity']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ['impact', 'uncoordinated', 'plugin', 'electric', 'vehicle', 'charging']\n",
      "Dataset URL: https://data.world/us-doe-gov/8ae7e117-313b-40b1-b146-83add97d400b\n",
      "Cosine Similarity: 0.7266891002655029\n",
      "\n",
      "Title: ['impact', 'uncoordinated', 'plugin', 'electric', 'vehicle', 'charging']\n",
      "Dataset URL: https://data.world/us-doe-gov/3f032a3c-7dc0-4f54-9f51-534b2e248f80\n",
      "Cosine Similarity: 0.7266891002655029\n",
      "\n",
      "Title: ['coronavirus', 'daily', 'data']\n",
      "Dataset URL: https://data.world/markmarkoh/coronavirus-data\n",
      "Cosine Similarity: 0.7453581094741821\n",
      "\n",
      "Title: ['fashion', 'images', 'dataset']\n",
      "Dataset URL: https://data.world/crawlfeeds/fashion-images-dataset\n",
      "Cosine Similarity: 0.74165940284729\n",
      "\n",
      "Title: ['site', 'g03', 'gasconade', 'river', 'bathymetry', 'structure', 'a1411', '89']\n",
      "Dataset URL: https://data.world/us-doi-gov/12749821-f445-4f35-8dee-81eb7a56f07d\n",
      "Cosine Similarity: 0.7338127493858337\n",
      "\n",
      "Title: ['twitter', 'dataset', '100', 'million', 'tweets', 'related', 'covid19']\n",
      "Dataset URL: https://data.world/rtekumalla1/a-twitter-dataset-of-100-million-tweets-related-to-covid-19\n",
      "Cosine Similarity: 0.7241174578666687\n",
      "\n",
      "Title: ['un', 'net', 'migration', 'rate', 'xls', '2']\n",
      "Dataset URL: https://data.world/brianray/un-net-migration-rate-xls-2\n",
      "Cosine Similarity: 0.7296226620674133\n",
      "\n",
      "Title: ['food', 'health', 'inspections']\n",
      "Dataset URL: https://data.world/durhamnc/food-health-inspection-data\n",
      "Cosine Similarity: 0.7372703552246094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraTokenizer, ElectraModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load ELECTRA tokenizer and model\n",
    "tokenizer = ElectraTokenizer.from_pretrained('google/electra-base-discriminator')\n",
    "model = ElectraModel.from_pretrained('google/electra-base-discriminator')\n",
    "model.eval()\n",
    "\n",
    "# Function to get ELECTRA embedding\n",
    "def get_electra_embedding(text, model, tokenizer, max_length=128):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=max_length)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use the last hidden state of the [CLS] token as the embedding\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "\n",
    "# Assuming MetaData_DS['description'] contains the dataset descriptions\n",
    "dataset_descriptions = Metadata_DS['text']\n",
    "\n",
    "# Generate embeddings for all dataset descriptions\n",
    "dataset_embeddings = []\n",
    "for description in dataset_descriptions:\n",
    "    emb = get_electra_embedding(description, model, tokenizer)\n",
    "    dataset_embeddings.append(emb)\n",
    "\n",
    "# Function to predict dataset URLs based on search text\n",
    "def predict_dataset_url(search_text):\n",
    "    search_emb = get_electra_embedding(search_text, model, tokenizer)\n",
    "    similarities = [cosine_similarity([search_emb], [dataset_emb])[0][0] for dataset_emb in dataset_embeddings]\n",
    "    \n",
    "    # Find the maximum and minimum similarity values\n",
    "    max_similarity = max(similarities)\n",
    "    min_similarity = min(similarities)\n",
    "    \n",
    "    # Calculate the 90% range\n",
    "    range_90 = 0.90 * (max_similarity - min_similarity)\n",
    "    \n",
    "    # Find the threshold value\n",
    "    threshold = min_similarity + range_90\n",
    "    \n",
    "    # Find the indices of datasets with similarity above the threshold\n",
    "    similar_indices = [i for i, sim in enumerate(similarities) if sim > threshold]\n",
    "    \n",
    "    results = []\n",
    "    for idx in similar_indices:\n",
    "        dataset_info = {\n",
    "            \"title\": Metadata_DS.iloc[idx]['title'],\n",
    "            \"dataset_url\": Metadata_DS.iloc[idx]['dataset_url'],\n",
    "            \"cosine_similarity\": similarities[idx]\n",
    "        }\n",
    "        results.append(dataset_info)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "search_text = \"electric vehicles\"\n",
    "results = predict_dataset_url(search_text)\n",
    "\n",
    "# Display results\n",
    "if not results:\n",
    "    print(\"No datasets found above the 90% threshold.\")\n",
    "else:\n",
    "    for result in results:\n",
    "        print(f\"Title: {result['title']}\")\n",
    "        print(f\"Dataset URL: {result['dataset_url']}\")\n",
    "        print(f\"Cosine Similarity: {result['cosine_similarity']}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
