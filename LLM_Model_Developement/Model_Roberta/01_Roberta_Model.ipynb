{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Packages Installation\n",
    "import pandas as pd\n",
    "from transformers import Trainer, TrainingArguments, T5ForConditionalGeneration, T5Tokenizer\n",
    "from datasets import load_metric\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Metadata = pd.read_csv('C:\\\\24592774_Dataset_Discovery_Using_LLM\\\\MetaData_Creation\\\\MetaData_Notebooks\\\\Prepared_MetaData_DataSet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MetaData_DS = Metadata.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MetaData_DS['text'] = MetaData_DS['title']+MetaData_DS['description']+MetaData_DS['summary']+MetaData_DS['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Temp\\ipykernel_27360\\2597346597.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_ids = torch.tensor(train_ids)\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Temp\\ipykernel_27360\\2597346597.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_ids = torch.tensor(val_ids)\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Temp\\ipykernel_27360\\2597346597.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_masks = torch.tensor(train_masks)\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Temp\\ipykernel_27360\\2597346597.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_masks = torch.tensor(val_masks)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 0.5205350458621979, Train Accuracy: 0.9701492537313433\n",
      "Validation Loss: 0.31941355764865875, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 2/10\n",
      "Train Loss: 0.17108711935579776, Train Accuracy: 1.0\n",
      "Validation Loss: 0.007925653364509344, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 3/10\n",
      "Train Loss: 0.01700754682533443, Train Accuracy: 1.0\n",
      "Validation Loss: 0.00048482517013326287, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 4/10\n",
      "Train Loss: 0.0010773394664283843, Train Accuracy: 1.0\n",
      "Validation Loss: 0.00012624956434592605, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 5/10\n",
      "Train Loss: 0.00033420291438233105, Train Accuracy: 1.0\n",
      "Validation Loss: 7.595577335450798e-05, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 6/10\n",
      "Train Loss: 0.0001720170897897333, Train Accuracy: 1.0\n",
      "Validation Loss: 4.248602635925636e-05, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 7/10\n",
      "Train Loss: 9.61590587394312e-05, Train Accuracy: 1.0\n",
      "Validation Loss: 2.8289452529861592e-05, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 8/10\n",
      "Train Loss: 8.493724890286103e-05, Train Accuracy: 1.0\n",
      "Validation Loss: 2.2288163563644048e-05, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 9/10\n",
      "Train Loss: 8.282496928586624e-05, Train Accuracy: 1.0\n",
      "Validation Loss: 1.8864692719944287e-05, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 10/10\n",
      "Train Loss: 4.267062759026885e-05, Train Accuracy: 1.0\n",
      "Validation Loss: 1.654015159147093e-05, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import RobertaTokenizer, RobertaModel, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Initialize RoBERTa tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Define maximum sequence length\n",
    "max_length = 128  # You can adjust this according to your dataset and memory constraints\n",
    "\n",
    "# Tokenize dataset descriptions\n",
    "tokenized_descriptions = tokenizer(list(MetaData_DS['description']), truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
    "\n",
    "# Convert labels to numerical format\n",
    "labels = [1 if url else 0 for url in MetaData_DS['dataset_url']]\n",
    "\n",
    "# Extract input_ids and attention_mask\n",
    "input_ids = tokenized_descriptions['input_ids']\n",
    "attention_mask = tokenized_descriptions['attention_mask']\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_ids, val_ids, train_masks, val_masks, train_labels, val_labels = train_test_split(input_ids, attention_mask, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert tokenized texts and labels to PyTorch tensors\n",
    "train_ids = torch.tensor(train_ids)\n",
    "val_ids = torch.tensor(val_ids)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "# Create TensorDataset for training and validation sets\n",
    "train_dataset = TensorDataset(train_ids, train_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_ids, val_masks, val_labels)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Create DataLoader for training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the RoBERTa-based model with increased dropout\n",
    "class RobertaClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(RobertaClassifier, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.dropout = nn.Dropout(0.3)  # Dropout rate\n",
    "        self.linear = nn.Linear(self.roberta.config.hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = outputs.last_hidden_state[:, 0]  # Take [CLS] token representation\n",
    "        hidden_state = self.dropout(hidden_state)\n",
    "        logits = self.linear(hidden_state)\n",
    "        return logits\n",
    "\n",
    "# Instantiate the RoBERTa model\n",
    "num_classes = 2  # Adjust according to your task\n",
    "model = RobertaClassifier(num_classes)\n",
    "\n",
    "# Define the optimizer with weight decay and gradient clipping\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=1, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop with early stopping\n",
    "epochs = 10\n",
    "patience = 2\n",
    "best_loss = float('inf')\n",
    "trigger_times = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for batch in train_loader:\n",
    "        inputs, attention_masks, labels = batch\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs, attention_masks)\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy for this batch\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = total_correct / total_samples\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, attention_masks, labels = batch\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs, attention_masks)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Compute predictions\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "            # Update total samples and total correct predictions\n",
    "            total_samples += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Save predictions and labels for metric calculation\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='binary')\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    print(f\"Train Loss: {avg_train_loss}, Train Accuracy: {train_accuracy}\")\n",
    "    print(f\"Validation Loss: {avg_val_loss}, Validation Accuracy: {accuracy}\")\n",
    "    print(f\"Validation Precision: {precision}, Validation Recall: {recall}, Validation F1 Score: {f1}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        trigger_times = 0\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print('Early stopping!')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Temp\\ipykernel_27360\\2901298168.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_ids = torch.tensor(train_ids)\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Temp\\ipykernel_27360\\2901298168.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_ids = torch.tensor(val_ids)\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Temp\\ipykernel_27360\\2901298168.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_masks = torch.tensor(train_masks)\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Temp\\ipykernel_27360\\2901298168.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_masks = torch.tensor(val_masks)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 0.2665253937244415, Train Accuracy: 1.0\n",
      "Validation Loss: 0.08833226561546326, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 2/10\n",
      "Train Loss: 0.03570154723711312, Train Accuracy: 1.0\n",
      "Validation Loss: 0.001009447529213503, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 3/10\n",
      "Train Loss: 0.0012741990038193762, Train Accuracy: 1.0\n",
      "Validation Loss: 0.00015762318798806518, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 4/10\n",
      "Train Loss: 0.00030561958847101777, Train Accuracy: 1.0\n",
      "Validation Loss: 6.998830212978646e-05, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 5/10\n",
      "Train Loss: 0.00021778157097287476, Train Accuracy: 1.0\n",
      "Validation Loss: 4.360729690233711e-05, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 6/10\n",
      "Train Loss: 0.00010036859166575595, Train Accuracy: 1.0\n",
      "Validation Loss: 3.167936574755004e-05, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 7/10\n",
      "Train Loss: 9.497818537056446e-05, Train Accuracy: 1.0\n",
      "Validation Loss: 2.632627911225427e-05, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 8/10\n",
      "Train Loss: 6.535333595820702e-05, Train Accuracy: 1.0\n",
      "Validation Loss: 2.4221541025326587e-05, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 9/10\n",
      "Train Loss: 5.6290784414159134e-05, Train Accuracy: 1.0\n",
      "Validation Loss: 2.1066291083116084e-05, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 10/10\n",
      "Train Loss: 5.438969674287364e-05, Train Accuracy: 1.0\n",
      "Validation Loss: 1.703933139651781e-05, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import RobertaTokenizer, RobertaModel, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Initialize RoBERTa tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Define maximum sequence length\n",
    "max_length = 128  # You can adjust this according to your dataset and memory constraints\n",
    "\n",
    "# Tokenize dataset descriptions\n",
    "tokenized_descriptions = tokenizer(list(MetaData_DS['text']), truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
    "\n",
    "# Convert labels to numerical format\n",
    "labels = [1 if url else 0 for url in MetaData_DS['dataset_url']]\n",
    "\n",
    "# Extract input_ids and attention_mask\n",
    "input_ids = tokenized_descriptions['input_ids']\n",
    "attention_mask = tokenized_descriptions['attention_mask']\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_ids, val_ids, train_masks, val_masks, train_labels, val_labels = train_test_split(input_ids, attention_mask, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert tokenized texts and labels to PyTorch tensors\n",
    "train_ids = torch.tensor(train_ids)\n",
    "val_ids = torch.tensor(val_ids)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "# Create TensorDataset for training and validation sets\n",
    "train_dataset = TensorDataset(train_ids, train_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_ids, val_masks, val_labels)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Create DataLoader for training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the RoBERTa-based model with increased dropout\n",
    "class RobertaClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(RobertaClassifier, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.dropout = nn.Dropout(0.3)  # Dropout rate\n",
    "        self.linear = nn.Linear(self.roberta.config.hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = outputs.last_hidden_state[:, 0]  # Take [CLS] token representation\n",
    "        hidden_state = self.dropout(hidden_state)\n",
    "        logits = self.linear(hidden_state)\n",
    "        return logits\n",
    "\n",
    "# Instantiate the RoBERTa model\n",
    "num_classes = 2  # Adjust according to your task\n",
    "model = RobertaClassifier(num_classes)\n",
    "\n",
    "# Define the optimizer with weight decay and gradient clipping\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=1, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop with early stopping\n",
    "epochs = 10\n",
    "patience = 2\n",
    "best_loss = float('inf')\n",
    "trigger_times = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for batch in train_loader:\n",
    "        inputs, attention_masks, labels = batch\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs, attention_masks)\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy for this batch\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = total_correct / total_samples\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, attention_masks, labels = batch\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs, attention_masks)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Compute predictions\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "            # Update total samples and total correct predictions\n",
    "            total_samples += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Save predictions and labels for metric calculation\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='binary')\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    print(f\"Train Loss: {avg_train_loss}, Train Accuracy: {train_accuracy}\")\n",
    "    print(f\"Validation Loss: {avg_val_loss}, Validation Accuracy: {accuracy}\")\n",
    "    print(f\"Validation Precision: {precision}, Validation Recall: {recall}, Validation F1 Score: {f1}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        trigger_times = 0\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print('Early stopping!')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ['test1234']\n",
      "Dataset URL: https://data.world/kgs1/test1234\n",
      "Cosine Similarity: 0.9991397261619568\n",
      "\n",
      "Title: ['legalmap']\n",
      "Dataset URL: https://data.world/h0tftw/legalmap\n",
      "Cosine Similarity: 0.9991397261619568\n",
      "\n",
      "Title: ['iptv', 'subscription', 'service', 'go']\n",
      "Dataset URL: https://data.world/freemotion/which-iptv-subscription-service-should-you-go-for\n",
      "Cosine Similarity: 0.9991397261619568\n",
      "\n",
      "Title: ['recognized', 'sports']\n",
      "Dataset URL: https://data.world/sports/recognized-sports\n",
      "Cosine Similarity: 0.9991397261619568\n",
      "\n",
      "Title: ['sports', 'illustrated', 'covers']\n",
      "Dataset URL: https://data.world/crowdflower/sports-illustrated-covers\n",
      "Cosine Similarity: 0.9991397261619568\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load RoBERTa tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "model.eval()\n",
    "\n",
    "# Function to get RoBERTa embedding\n",
    "def get_roberta_embedding(text, model, tokenizer, max_length=128):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=max_length)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use the last hidden state of the [CLS] token as the embedding\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "\n",
    "# Assuming MetaData_DS['description'] contains the dataset descriptions\n",
    "dataset_descriptions = MetaData_DS['description']\n",
    "\n",
    "# Generate embeddings for all dataset descriptions\n",
    "dataset_embeddings = []\n",
    "for description in dataset_descriptions:\n",
    "    emb = get_roberta_embedding(description, model, tokenizer)\n",
    "    dataset_embeddings.append(emb)\n",
    "\n",
    "# Function to predict dataset URLs based on search text\n",
    "def predict_dataset_url(search_text):\n",
    "    search_emb = get_roberta_embedding(search_text, model, tokenizer)\n",
    "    similarities = [cosine_similarity([search_emb], [dataset_emb])[0][0] for dataset_emb in dataset_embeddings]\n",
    "    \n",
    "    # Find the maximum and minimum similarity values\n",
    "    max_similarity = max(similarities)\n",
    "    min_similarity = min(similarities)\n",
    "    \n",
    "    # Calculate the 90% range\n",
    "    range_90 = 0.90 * (max_similarity - min_similarity)\n",
    "    \n",
    "    # Find the threshold value\n",
    "    threshold = min_similarity + range_90\n",
    "    \n",
    "    # Find the indices of datasets with similarity above the threshold\n",
    "    similar_indices = [i for i, sim in enumerate(similarities) if sim > threshold]\n",
    "    \n",
    "    results = []\n",
    "    for idx in similar_indices:\n",
    "        dataset_info = {\n",
    "            \"title\": MetaData_DS.iloc[idx]['title'],\n",
    "            \"dataset_url\": MetaData_DS.iloc[idx]['dataset_url'],\n",
    "            \"cosine_similarity\": similarities[idx]\n",
    "        }\n",
    "        results.append(dataset_info)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "search_text = \"electric vehicles\"\n",
    "results = predict_dataset_url(search_text)\n",
    "\n",
    "# Display results\n",
    "if not results:\n",
    "    print(\"No datasets found above the 90% threshold.\")\n",
    "else:\n",
    "    for result in results:\n",
    "        print(f\"Title: {result['title']}\")\n",
    "        print(f\"Dataset URL: {result['dataset_url']}\")\n",
    "        print(f\"Cosine Similarity: {result['cosine_similarity']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ['electric', 'vehicle', 'year']\n",
      "Dataset URL: https://data.world/jeffgswanson/electric-vehicle-by-year\n",
      "Cosine Similarity: 0.996622622013092\n",
      "\n",
      "Title: ['cost', 'benchmarking']\n",
      "Dataset URL: https://data.world/george-t-stagg/cost-benchmarking\n",
      "Cosine Similarity: 0.9964325428009033\n",
      "\n",
      "Title: ['test1234']\n",
      "Dataset URL: https://data.world/kgs1/test1234\n",
      "Cosine Similarity: 0.9962551593780518\n",
      "\n",
      "Title: ['legalmap']\n",
      "Dataset URL: https://data.world/h0tftw/legalmap\n",
      "Cosine Similarity: 0.9963016510009766\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load RoBERTa tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "model.eval()\n",
    "\n",
    "# Function to get RoBERTa embedding\n",
    "def get_roberta_embedding(text, model, tokenizer, max_length=128):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=max_length)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use the last hidden state of the [CLS] token as the embedding\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "\n",
    "# Assuming MetaData_DS['description'] contains the dataset descriptions\n",
    "dataset_descriptions = MetaData_DS['text']\n",
    "\n",
    "# Generate embeddings for all dataset descriptions\n",
    "dataset_embeddings = []\n",
    "for description in dataset_descriptions:\n",
    "    emb = get_roberta_embedding(description, model, tokenizer)\n",
    "    dataset_embeddings.append(emb)\n",
    "\n",
    "# Function to predict dataset URLs based on search text\n",
    "def predict_dataset_url(search_text):\n",
    "    search_emb = get_roberta_embedding(search_text, model, tokenizer)\n",
    "    similarities = [cosine_similarity([search_emb], [dataset_emb])[0][0] for dataset_emb in dataset_embeddings]\n",
    "    \n",
    "    # Find the maximum and minimum similarity values\n",
    "    max_similarity = max(similarities)\n",
    "    min_similarity = min(similarities)\n",
    "    \n",
    "    # Calculate the 90% range\n",
    "    range_90 = 0.90 * (max_similarity - min_similarity)\n",
    "    \n",
    "    # Find the threshold value\n",
    "    threshold = min_similarity + range_90\n",
    "    \n",
    "    # Find the indices of datasets with similarity above the threshold\n",
    "    similar_indices = [i for i, sim in enumerate(similarities) if sim > threshold]\n",
    "    \n",
    "    results = []\n",
    "    for idx in similar_indices:\n",
    "        dataset_info = {\n",
    "            \"title\": MetaData_DS.iloc[idx]['title'],\n",
    "            \"dataset_url\": MetaData_DS.iloc[idx]['dataset_url'],\n",
    "            \"cosine_similarity\": similarities[idx]\n",
    "        }\n",
    "        results.append(dataset_info)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "search_text = \"electric vehicles charging\"\n",
    "results = predict_dataset_url(search_text)\n",
    "\n",
    "# Display results\n",
    "if not results:\n",
    "    print(\"No datasets found above the 90% threshold.\")\n",
    "else:\n",
    "    for result in results:\n",
    "        print(f\"Title: {result['title']}\")\n",
    "        print(f\"Dataset URL: {result['dataset_url']}\")\n",
    "        print(f\"Cosine Similarity: {result['cosine_similarity']}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
