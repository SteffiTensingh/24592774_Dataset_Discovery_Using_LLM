{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Metadata = pd.read_csv('C:\\\\Users\\\\Steffi Grace\\\\24592774_LLM_ILabResearch\\\\Notebooks\\\\Datasets\\\\Filtered_MetaData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed metadata:\n",
      "                                               title  \\\n",
      "0  [2023w7, global, electric, vehicle, market, sh...   \n",
      "1         [connected, electric, autonomous, vehicle]   \n",
      "2            [electric, vehicle, charging, stations]   \n",
      "3  [nyserda, electric, vehicle, drive, clean, reb...   \n",
      "4  [impact, of, uncoordinated, plugin, electric, ...   \n",
      "\n",
      "                                         description  \\\n",
      "0         [global, electric, vehicle, market, share]   \n",
      "1         [connected, electric, autonomous, vehicle]   \n",
      "2            [electric, vehicle, charging, stations]   \n",
      "3  [new, york, states, charge, ny, initiative, of...   \n",
      "4  [impact, of, uncoordinated, plugin, electric, ...   \n",
      "\n",
      "                                             summary  \\\n",
      "0         [global, electric, vehicle, market, share]   \n",
      "1  [romanian, new, car, registration, in, 2023, v...   \n",
      "2  [statewise, data, from, 2001, is, classified, ...   \n",
      "3      [coronoavirus, covid19, data, updated, daily]   \n",
      "4  [adding, public, datasets, related, to, corona...   \n",
      "\n",
      "                                                tags  \\\n",
      "0  makeover monday cars vehicles electric ev mark...   \n",
      "1                                         autonomous   \n",
      "2                electric vehicle environment energy   \n",
      "3  ev electric vehicle bev phev ghg drive clean r...   \n",
      "4  battery consumption data energy hybrid midwest...   \n",
      "\n",
      "                                         dataset_url  \\\n",
      "0           https://data.world/makeovermonday/2023w7   \n",
      "1  https://data.world/smartcolumbusos/650b7e59-af...   \n",
      "2  https://data.world/townofcary/electric-vehicle...   \n",
      "3           https://data.world/data-ny-gov/thd2-fu8y   \n",
      "4  https://data.world/us-doe-gov/8ae7e117-313b-40...   \n",
      "\n",
      "                             available_formats  \n",
      "0                                     ['xlsx']  \n",
      "1                                      ['csv']  \n",
      "2  ['dbf', 'prj', 'json', 'shp', 'csv', 'shx']  \n",
      "3                                      ['csv']  \n",
      "4                                     ['xlsx']  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "def preprocess_metadata(metadata):\n",
    "    # Clean text data (e.g., remove special characters, lowercase)\n",
    "    cleaned_metadata = metadata.copy() \n",
    "    cleaned_metadata['description'] = cleaned_metadata['description'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', str(x)))\n",
    "    cleaned_metadata['description'] = cleaned_metadata['description'].str.lower()\n",
    "    cleaned_metadata['title'] = cleaned_metadata['title'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', str(x)))\n",
    "    cleaned_metadata['title'] = cleaned_metadata['title'].str.lower()\n",
    "    cleaned_metadata['summary'] = cleaned_metadata['summary'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', str(x)))\n",
    "    cleaned_metadata['summary'] = cleaned_metadata['summary'].str.lower()\n",
    "\n",
    "    \n",
    "    # Tokenize text data (split into words)\n",
    "    cleaned_metadata['description'] = cleaned_metadata['description'].str.split()        \n",
    "    cleaned_metadata['title'] = cleaned_metadata['title'].str.split()        \n",
    "    cleaned_metadata['summary'] = cleaned_metadata['summary'].str.split()\n",
    "\n",
    "    # Handle missing values (e.g., fill with a placeholder or remove rows)\n",
    "    cleaned_metadata.fillna('', inplace=True)\n",
    "    \n",
    "    return cleaned_metadata\n",
    "\n",
    "# Preprocess metadata\n",
    "Preprocess_Metadata = preprocess_metadata(Metadata)\n",
    "\n",
    "print(\"Preprocessed metadata:\")\n",
    "print(Preprocess_Metadata.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Replace 'nan' string with an empty string\n",
    "    if text == 'nan':\n",
    "        return \"\"\n",
    "    if isinstance(text, list):\n",
    "        # Convert list of strings to a single string\n",
    "        text = ' '.join(text)\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove punctuation and non-alphabetic characters, and single character tokens\n",
    "    tokens = [re.sub(r'[^a-zA-Z]', '', token) for token in tokens if token.isalpha() and len(token) > 1]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatize tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Join tokens back into a single string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>summary</th>\n",
       "      <th>tags</th>\n",
       "      <th>dataset_url</th>\n",
       "      <th>available_formats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[2023w7, global, electric, vehicle, market, sh...</td>\n",
       "      <td>[global, electric, vehicle, market, share]</td>\n",
       "      <td>[global, electric, vehicle, market, share]</td>\n",
       "      <td>makeover monday cars vehicles electric ev mark...</td>\n",
       "      <td>https://data.world/makeovermonday/2023w7</td>\n",
       "      <td>['xlsx']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[connected, electric, autonomous, vehicle]</td>\n",
       "      <td>[connected, electric, autonomous, vehicle]</td>\n",
       "      <td>[romanian, new, car, registration, in, 2023, v...</td>\n",
       "      <td>autonomous</td>\n",
       "      <td>https://data.world/smartcolumbusos/650b7e59-af...</td>\n",
       "      <td>['csv']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[electric, vehicle, charging, stations]</td>\n",
       "      <td>[electric, vehicle, charging, stations]</td>\n",
       "      <td>[statewise, data, from, 2001, is, classified, ...</td>\n",
       "      <td>electric vehicle environment energy</td>\n",
       "      <td>https://data.world/townofcary/electric-vehicle...</td>\n",
       "      <td>['dbf', 'prj', 'json', 'shp', 'csv', 'shx']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[nyserda, electric, vehicle, drive, clean, reb...</td>\n",
       "      <td>[new, york, states, charge, ny, initiative, of...</td>\n",
       "      <td>[coronoavirus, covid19, data, updated, daily]</td>\n",
       "      <td>ev electric vehicle bev phev ghg drive clean r...</td>\n",
       "      <td>https://data.world/data-ny-gov/thd2-fu8y</td>\n",
       "      <td>['csv']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[impact, of, uncoordinated, plugin, electric, ...</td>\n",
       "      <td>[impact, of, uncoordinated, plugin, electric, ...</td>\n",
       "      <td>[adding, public, datasets, related, to, corona...</td>\n",
       "      <td>battery consumption data energy hybrid midwest...</td>\n",
       "      <td>https://data.world/us-doe-gov/8ae7e117-313b-40...</td>\n",
       "      <td>['xlsx']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  [2023w7, global, electric, vehicle, market, sh...   \n",
       "1         [connected, electric, autonomous, vehicle]   \n",
       "2            [electric, vehicle, charging, stations]   \n",
       "3  [nyserda, electric, vehicle, drive, clean, reb...   \n",
       "4  [impact, of, uncoordinated, plugin, electric, ...   \n",
       "\n",
       "                                         description  \\\n",
       "0         [global, electric, vehicle, market, share]   \n",
       "1         [connected, electric, autonomous, vehicle]   \n",
       "2            [electric, vehicle, charging, stations]   \n",
       "3  [new, york, states, charge, ny, initiative, of...   \n",
       "4  [impact, of, uncoordinated, plugin, electric, ...   \n",
       "\n",
       "                                             summary  \\\n",
       "0         [global, electric, vehicle, market, share]   \n",
       "1  [romanian, new, car, registration, in, 2023, v...   \n",
       "2  [statewise, data, from, 2001, is, classified, ...   \n",
       "3      [coronoavirus, covid19, data, updated, daily]   \n",
       "4  [adding, public, datasets, related, to, corona...   \n",
       "\n",
       "                                                tags  \\\n",
       "0  makeover monday cars vehicles electric ev mark...   \n",
       "1                                         autonomous   \n",
       "2                electric vehicle environment energy   \n",
       "3  ev electric vehicle bev phev ghg drive clean r...   \n",
       "4  battery consumption data energy hybrid midwest...   \n",
       "\n",
       "                                         dataset_url  \\\n",
       "0           https://data.world/makeovermonday/2023w7   \n",
       "1  https://data.world/smartcolumbusos/650b7e59-af...   \n",
       "2  https://data.world/townofcary/electric-vehicle...   \n",
       "3           https://data.world/data-ny-gov/thd2-fu8y   \n",
       "4  https://data.world/us-doe-gov/8ae7e117-313b-40...   \n",
       "\n",
       "                             available_formats  \n",
       "0                                     ['xlsx']  \n",
       "1                                      ['csv']  \n",
       "2  ['dbf', 'prj', 'json', 'shp', 'csv', 'shx']  \n",
       "3                                      ['csv']  \n",
       "4                                     ['xlsx']  "
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Preprocess_Metadata.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Apply preprocessing to text columns\n",
    "for column in ['title', 'description', 'summary']:\n",
    "    Preprocess_Metadata[column] = Preprocess_Metadata[column].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant dataset URLs sorted by similarity score (descending order):\n",
      "---------------------------------\n",
      "        Search text              \n",
      "---------------------------------\n",
      "Corona Virus\n",
      "---------------------------------\n",
      "                                 \n",
      "---------------------------------\n",
      "Counter: 1\n",
      "Similarity Score: 0.7031020998954773\n",
      "Dataset URL: https://data.world/markmarkoh/coronavirus-data\n",
      "Title: coronavirus daily data\n",
      "Description: coronoavirus data updated daily\n",
      "Summary: coronoavirus data updated daily\n",
      "\n",
      "\n",
      "Counter: 2\n",
      "Similarity Score: 0.6646250367164612\n",
      "Dataset URL: https://data.world/chandrasekar/coronatracking\n",
      "Title: coronatracking\n",
      "Description: adding public datasets related corona virus update\n",
      "Summary: adding public datasets related corona virus update\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# Load pre-trained T5 model and tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "# Assuming Preprocess_Metadata is the DataFrame containing the dataset\n",
    "metadata = Preprocess_Metadata.copy()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if text == 'nan':\n",
    "        return \"\"\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [re.sub(r'[^a-zA-Z]', '', token) for token in tokens if token.isalpha() and len(token) > 1]\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        preprocessed_text = ' '.join(tokens)\n",
    "        return preprocessed_text\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "for column in ['title', 'description', 'summary']:\n",
    "    metadata[column] = metadata[column].apply(preprocess_text)\n",
    "\n",
    "metadata_embeddings = []\n",
    "for index, row in metadata.iterrows():\n",
    "    text = row['title'] + ' ' + row['description'] + ' ' + row['summary']\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids)\n",
    "    embeddings = model.get_input_embeddings()(input_ids).mean(dim=1).squeeze().detach().numpy()\n",
    "    \n",
    "\n",
    "    metadata_embeddings.append(embeddings)\n",
    "\n",
    "user_input = input(\"Enter your query: \")\n",
    "input_ids = tokenizer.encode(user_input, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "with torch.no_grad():\n",
    "    output = model.generate(input_ids)\n",
    "user_embeddings = model.get_input_embeddings()(input_ids).mean(dim=1).squeeze().detach().numpy()\n",
    "\n",
    "\n",
    "similarities = cosine_similarity([user_embeddings], metadata_embeddings)[0]\n",
    "\n",
    "def adjust_score(score, title, description, summary):\n",
    "    completeness_factor = 0\n",
    "    if title:\n",
    "        completeness_factor += 0.1\n",
    "    if description:\n",
    "        completeness_factor += 0.1\n",
    "    if summary:\n",
    "        completeness_factor += 0.1\n",
    "    return score + completeness_factor\n",
    "\n",
    "adjusted_similarities = []\n",
    "for i, score in enumerate(similarities):\n",
    "    title, description, summary = metadata.iloc[i]['title'], metadata.iloc[i]['description'], metadata.iloc[i]['summary']\n",
    "    adjusted_score = adjust_score(score, title, description, summary)\n",
    "    adjusted_similarities.append(adjusted_score)\n",
    "\n",
    "# Calculate the 95th percentile threshold\n",
    "percentile_threshold = np.percentile(adjusted_similarities, 98)\n",
    "\n",
    "dataset_urls_with_scores = zip(metadata['dataset_url'], adjusted_similarities)\n",
    "sorted_dataset_urls_with_scores = sorted(dataset_urls_with_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Relevant dataset URLs sorted by similarity score (descending order):\")\n",
    "print(\"---------------------------------\")\n",
    "print(\"        Search text              \")\n",
    "print(\"---------------------------------\")\n",
    "print(user_input)\n",
    "print(\"---------------------------------\")\n",
    "print(\"                                 \")\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "\n",
    "for counter, (dataset_url, similarity_score) in enumerate(sorted_dataset_urls_with_scores, start=1):\n",
    "    if similarity_score >= percentile_threshold:\n",
    "        print(f\"Counter: {counter}\")\n",
    "        print(f\"Similarity Score: {similarity_score}\")\n",
    "        print(f\"Dataset URL: {dataset_url}\")\n",
    "        metadata_index = metadata.index[metadata['dataset_url'] == dataset_url].tolist()[0]\n",
    "        print(f\"Title: {metadata['title'].iloc[metadata_index]}\")\n",
    "        print(f\"Description: {metadata['description'].iloc[metadata_index]}\")\n",
    "        print(f\"Summary: {metadata['summary'].iloc[metadata_index]}\")\n",
    "        print(\"\\n\")\n",
    "        total_output = counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "79\n"
     ]
    }
   ],
   "source": [
    "print(len(metadata))\n",
    "print(len(similarities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the labeled metadata with label == 1\n",
    "New_Metadata = pd.read_csv('C:\\\\Users\\\\Steffi Grace\\\\24592774_LLM_ILabResearch\\\\Notebooks\\\\Datasets\\\\New_Prep_Metadata.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed metadata:\n",
      "        owner                                    id  \\\n",
      "0         hdx  80ded4e2-aa4f-42bc-ad08-6c1effc24123   \n",
      "1         hdx  f92954dc-3f5b-407a-80a9-1e178280b0d7   \n",
      "2  us-hhs-gov  87c842cf-095a-412e-8cf2-5d87ed07e34b   \n",
      "3   alexandra                    food-related-words   \n",
      "4        chhs  0114f5bb-4975-419d-95d9-5f0179a8de06   \n",
      "\n",
      "                                               title  \\\n",
      "0  [europe, coronavirus, covid19, subnational, ca...   \n",
      "1  [the, new, york, times, coronavirus, covid19, ...   \n",
      "2  [provisional, death, counts, for, coronavirus,...   \n",
      "3                             [food, related, words]   \n",
      "4                              [food, affordability]   \n",
      "\n",
      "                                         description  \\\n",
      "0  [europe, coronavirus, covid19, subnational, ca...   \n",
      "1  [the, new, york, times, coronavirus, covid19, ...   \n",
      "2  [provisional, death, counts, for, coronavirus,...   \n",
      "3  [7000, words, that, relate, to, food, producti...   \n",
      "4  [this, table, contains, data, on, the, average...   \n",
      "\n",
      "                                             summary  \\\n",
      "0  [this, dataset, contains, the, number, of, con...   \n",
      "1  [original, title, the, new, york, times, coron...   \n",
      "2  [original, title, provisional, death, counts, ...   \n",
      "3  [manually, checked, only, once, so, not, entir...   \n",
      "4  [this, table, contains, data, on, the, average...   \n",
      "\n",
      "                                version  \\\n",
      "0  236eb740-41df-4af8-965e-fad2e25875e1   \n",
      "1  e4d940d6-e6a3-44e5-8abc-2be9a72ded19   \n",
      "2  5cf4bb3e-ce0e-4a5d-971b-c071b99fd7ef   \n",
      "3  a554330f-97fa-4bf5-b857-81577234d39e   \n",
      "4  d3072b9c-b1fe-4088-aa05-2d696f608198   \n",
      "\n",
      "                                                tags        license  \\\n",
      "0  ['affected', 'population', 'epidemics', 'and',...          CC-BY   \n",
      "1  ['affected', 'population', 'epidemics', 'and',...          Other   \n",
      "2  ['coronavirus', 'deaths', 'ethnicity', 'mortal...  Public Domain   \n",
      "3                    ['food', 'words', 'dictionary']  Public Domain   \n",
      "4  ['food', 'affordability', 'food', 'security', ...          Other   \n",
      "\n",
      "  visibility                                              files  status  \\\n",
      "0       OPEN  [{'name': 'europe-coronavirus-covid-19-subnati...  LOADED   \n",
      "1       OPEN  [{'name': 'us-counties-csv-2.csv', 'sizeInByte...  LOADED   \n",
      "2       OPEN  [{'name': 'csv-1.csv', 'sizeInBytes': 23167, '...  LOADED   \n",
      "3       OPEN  [{'name': 'food-related.csv', 'sizeInBytes': 8...  LOADED   \n",
      "4       OPEN  [{'name': 'datapackage.json', 'sizeInBytes': 8...  LOADED   \n",
      "\n",
      "                    created                   updated accessLevel versionDois  \\\n",
      "0  2020-04-29T16:56:30.697Z  2020-09-25T14:10:46.177Z        READ          []   \n",
      "1  2020-04-04T15:54:08.726Z  2020-05-22T23:43:03.191Z        READ          []   \n",
      "2  2020-06-25T15:53:14.617Z  2020-06-25T15:53:15.436Z        READ          []   \n",
      "3  2018-04-02T12:01:01.924Z  2018-04-02T12:06:05.006Z        READ          []   \n",
      "4  2018-09-19T02:27:53.203Z  2020-10-07T15:54:15.362Z        READ          []   \n",
      "\n",
      "   isProject                                        dataset_url  \\\n",
      "0      False  https://data.world/hdx/80ded4e2-aa4f-42bc-ad08...   \n",
      "1      False  https://data.world/hdx/f92954dc-3f5b-407a-80a9...   \n",
      "2      False  https://data.world/us-hhs-gov/87c842cf-095a-41...   \n",
      "3      False    https://data.world/alexandra/food-related-words   \n",
      "4      False  https://data.world/chhs/0114f5bb-4975-419d-95d...   \n",
      "\n",
      "         available_formats  \n",
      "0                  ['csv']  \n",
      "1                  ['csv']  \n",
      "2                  ['csv']  \n",
      "3                  ['csv']  \n",
      "4  ['xlsx', 'json', 'pdf']  \n"
     ]
    }
   ],
   "source": [
    "def preprocess_metadata(metadata):\n",
    "    # Clean text data (e.g., remove special characters, lowercase)\n",
    "    cleaned_metadata = New_Metadata.copy() \n",
    "    cleaned_metadata['description'] = cleaned_metadata['description'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', str(x)))\n",
    "    cleaned_metadata['description'] = cleaned_metadata['description'].str.lower()\n",
    "    cleaned_metadata['title'] = cleaned_metadata['title'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', str(x)))\n",
    "    cleaned_metadata['title'] = cleaned_metadata['title'].str.lower()\n",
    "    cleaned_metadata['summary'] = cleaned_metadata['summary'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', str(x)))\n",
    "    cleaned_metadata['summary'] = cleaned_metadata['summary'].str.lower()\n",
    "\n",
    "    \n",
    "    # Tokenize text data (split into words)\n",
    "    cleaned_metadata['description'] = cleaned_metadata['description'].str.split()        \n",
    "    cleaned_metadata['title'] = cleaned_metadata['title'].str.split()        \n",
    "    cleaned_metadata['summary'] = cleaned_metadata['summary'].str.split()\n",
    "\n",
    "    # Handle missing values (e.g., fill with a placeholder or remove rows)\n",
    "    cleaned_metadata.fillna('', inplace=True)\n",
    "    \n",
    "    return cleaned_metadata\n",
    "\n",
    "# Preprocess metadata\n",
    "Preprocess_Metadata = preprocess_metadata(Metadata)\n",
    "\n",
    "print(\"Preprocessed metadata:\")\n",
    "print(Preprocess_Metadata.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filtered_cols = ['title','description','summary','tags','dataset_url','available_formats']\n",
    "New_Metadata = Preprocess_Metadata[Filtered_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>summary</th>\n",
       "      <th>tags</th>\n",
       "      <th>dataset_url</th>\n",
       "      <th>available_formats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[europe, coronavirus, covid19, subnational, ca...</td>\n",
       "      <td>[europe, coronavirus, covid19, subnational, ca...</td>\n",
       "      <td>[this, dataset, contains, the, number, of, con...</td>\n",
       "      <td>['affected', 'population', 'epidemics', 'and',...</td>\n",
       "      <td>https://data.world/hdx/80ded4e2-aa4f-42bc-ad08...</td>\n",
       "      <td>['csv']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[the, new, york, times, coronavirus, covid19, ...</td>\n",
       "      <td>[the, new, york, times, coronavirus, covid19, ...</td>\n",
       "      <td>[original, title, the, new, york, times, coron...</td>\n",
       "      <td>['affected', 'population', 'epidemics', 'and',...</td>\n",
       "      <td>https://data.world/hdx/f92954dc-3f5b-407a-80a9...</td>\n",
       "      <td>['csv']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[provisional, death, counts, for, coronavirus,...</td>\n",
       "      <td>[provisional, death, counts, for, coronavirus,...</td>\n",
       "      <td>[original, title, provisional, death, counts, ...</td>\n",
       "      <td>['coronavirus', 'deaths', 'ethnicity', 'mortal...</td>\n",
       "      <td>https://data.world/us-hhs-gov/87c842cf-095a-41...</td>\n",
       "      <td>['csv']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[food, related, words]</td>\n",
       "      <td>[7000, words, that, relate, to, food, producti...</td>\n",
       "      <td>[manually, checked, only, once, so, not, entir...</td>\n",
       "      <td>['food', 'words', 'dictionary']</td>\n",
       "      <td>https://data.world/alexandra/food-related-words</td>\n",
       "      <td>['csv']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[food, affordability]</td>\n",
       "      <td>[this, table, contains, data, on, the, average...</td>\n",
       "      <td>[this, table, contains, data, on, the, average...</td>\n",
       "      <td>['food', 'affordability', 'food', 'security', ...</td>\n",
       "      <td>https://data.world/chhs/0114f5bb-4975-419d-95d...</td>\n",
       "      <td>['xlsx', 'json', 'pdf']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  [europe, coronavirus, covid19, subnational, ca...   \n",
       "1  [the, new, york, times, coronavirus, covid19, ...   \n",
       "2  [provisional, death, counts, for, coronavirus,...   \n",
       "3                             [food, related, words]   \n",
       "4                              [food, affordability]   \n",
       "\n",
       "                                         description  \\\n",
       "0  [europe, coronavirus, covid19, subnational, ca...   \n",
       "1  [the, new, york, times, coronavirus, covid19, ...   \n",
       "2  [provisional, death, counts, for, coronavirus,...   \n",
       "3  [7000, words, that, relate, to, food, producti...   \n",
       "4  [this, table, contains, data, on, the, average...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  [this, dataset, contains, the, number, of, con...   \n",
       "1  [original, title, the, new, york, times, coron...   \n",
       "2  [original, title, provisional, death, counts, ...   \n",
       "3  [manually, checked, only, once, so, not, entir...   \n",
       "4  [this, table, contains, data, on, the, average...   \n",
       "\n",
       "                                                tags  \\\n",
       "0  ['affected', 'population', 'epidemics', 'and',...   \n",
       "1  ['affected', 'population', 'epidemics', 'and',...   \n",
       "2  ['coronavirus', 'deaths', 'ethnicity', 'mortal...   \n",
       "3                    ['food', 'words', 'dictionary']   \n",
       "4  ['food', 'affordability', 'food', 'security', ...   \n",
       "\n",
       "                                         dataset_url        available_formats  \n",
       "0  https://data.world/hdx/80ded4e2-aa4f-42bc-ad08...                  ['csv']  \n",
       "1  https://data.world/hdx/f92954dc-3f5b-407a-80a9...                  ['csv']  \n",
       "2  https://data.world/us-hhs-gov/87c842cf-095a-41...                  ['csv']  \n",
       "3    https://data.world/alexandra/food-related-words                  ['csv']  \n",
       "4  https://data.world/chhs/0114f5bb-4975-419d-95d...  ['xlsx', 'json', 'pdf']  "
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "New_Metadata.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Temp\\ipykernel_28004\\2178811352.py:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metadata = metadata.append(New_Metadata, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Assuming new_data contains the new dataset to be added\n",
    "new_data_embeddings = []\n",
    "for index, row in New_Metadata.iterrows():\n",
    "    # Join the elements of the lists into a single string\n",
    "    text = ' '.join(row['title']) + ' ' + ' '.join(row['description']) + ' ' + ' '.join(row['summary'])\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids)\n",
    "    embeddings = model.get_input_embeddings()(input_ids).mean(dim=1).squeeze().detach().numpy()\n",
    "    new_data_embeddings.append(embeddings)\n",
    "\n",
    "# Append new data embeddings to existing metadata embeddings\n",
    "#metadata_embeddings.extend(new_data_embeddings)\n",
    "metadata = metadata.append(New_Metadata, ignore_index=True)\n",
    "index_start = len(metadata_embeddings)\n",
    "metadata_embeddings[index_start:] = new_data_embeddings\n",
    "\n",
    "# Compute similarities with the extended metadata embeddings\n",
    "similarities = cosine_similarity([user_embeddings], metadata_embeddings)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "5\n",
      "84\n",
      "84\n"
     ]
    }
   ],
   "source": [
    "print(len(metadata))\n",
    "print(len(New_Metadata))\n",
    "print(len(similarities))\n",
    "print(len(metadata_embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant dataset URLs sorted by similarity score (descending order):\n",
      "---------------------------------\n",
      "        Search text              \n",
      "---------------------------------\n",
      "Food related words\n",
      "---------------------------------\n",
      "                                 \n",
      "---------------------------------\n",
      "Counter: 1\n",
      "Similarity Score: 0.8246365070343018\n",
      "Dataset URL: https://data.world/alexandra/food-related-words\n",
      "Title: ['food', 'related', 'words']\n",
      "Description: ['7000', 'words', 'that', 'relate', 'to', 'food', 'production', 'of', 'food', 'cooking', 'ingredients', 'groceries', 'markets', 'etc', 'single', 'column', 'csv']\n",
      "Summary: ['manually', 'checked', 'only', 'once', 'so', 'not', 'entirely', 'clean', 'as', 'it', 'contains', 'common', 'english', 'nouns', 'as', 'well', 'these', 'can', 'be', 'easily', 'sorted', 'out', 'however', 'some', 'words', 'are', 'indeed', 'food', 'related', 'such', 'as', 'brusselssprouts', 'or', 'whole', 'wheat']\n",
      "\n",
      "\n",
      "Counter: 2\n",
      "Similarity Score: 0.7313822984695435\n",
      "Dataset URL: https://data.world/agriculture/food-environment-atlas\n",
      "Title: food environment atlas\n",
      "Description: statistic food environment indicator stimulate research determinant food choice diet quality\n",
      "Summary: food environment factor storerestaurant proximity food price food nutrition assistance program community characteristic interact influence food choice diet quality research beginning document complexity interaction needed identify causal relationship effective policy intervention objective atlas assemble statistic food environment indicator stimulate research determinant food choice diet quality provide spatial overview community ability access healthy food success includes current past archived data attribution department agriculture economic research\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\generation\\utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "user_input = input(\"Enter your query: \")\n",
    "input_ids = tokenizer.encode(user_input, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "with torch.no_grad():\n",
    "    output = model.generate(input_ids)\n",
    "user_embeddings = model.get_input_embeddings()(input_ids).mean(dim=1).squeeze().detach().numpy()\n",
    "\n",
    "\n",
    "similarities = cosine_similarity([user_embeddings], metadata_embeddings)[0]\n",
    "\n",
    "def adjust_score(score, title, description, summary):\n",
    "    completeness_factor = 0\n",
    "    if title:\n",
    "        completeness_factor += 0.1\n",
    "    if description:\n",
    "        completeness_factor += 0.1\n",
    "    if summary:\n",
    "        completeness_factor += 0.1\n",
    "    return score + completeness_factor\n",
    "\n",
    "adjusted_similarities = []\n",
    "for i, score in enumerate(similarities):\n",
    "    title, description, summary = metadata.iloc[i]['title'], metadata.iloc[i]['description'], metadata.iloc[i]['summary']\n",
    "    adjusted_score = adjust_score(score, title, description, summary)\n",
    "    adjusted_similarities.append(adjusted_score)\n",
    "\n",
    "# Calculate the 95th percentile threshold\n",
    "percentile_threshold = np.percentile(adjusted_similarities, 98)\n",
    "\n",
    "dataset_urls_with_scores = zip(metadata['dataset_url'], adjusted_similarities)\n",
    "sorted_dataset_urls_with_scores = sorted(dataset_urls_with_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Relevant dataset URLs sorted by similarity score (descending order):\")\n",
    "print(\"---------------------------------\")\n",
    "print(\"        Search text              \")\n",
    "print(\"---------------------------------\")\n",
    "print(user_input)\n",
    "print(\"---------------------------------\")\n",
    "print(\"                                 \")\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "\n",
    "for counter, (dataset_url, similarity_score) in enumerate(sorted_dataset_urls_with_scores, start=1):\n",
    "    if similarity_score >= percentile_threshold:\n",
    "        print(f\"Counter: {counter}\")\n",
    "        print(f\"Similarity Score: {similarity_score}\")\n",
    "        print(f\"Dataset URL: {dataset_url}\")\n",
    "        metadata_index = metadata.index[metadata['dataset_url'] == dataset_url].tolist()[0]\n",
    "        print(f\"Title: {metadata['title'].iloc[metadata_index]}\")\n",
    "        print(f\"Description: {metadata['description'].iloc[metadata_index]}\")\n",
    "        print(f\"Summary: {metadata['summary'].iloc[metadata_index]}\")\n",
    "        print(\"\\n\")\n",
    "        total_output = counter\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
