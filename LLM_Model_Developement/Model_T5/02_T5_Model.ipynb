{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Packages Installation\n",
    "import pandas as pd\n",
    "from transformers import Trainer, TrainingArguments, T5ForConditionalGeneration, T5Tokenizer\n",
    "from datasets import load_metric\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Metadata = pd.read_csv('C:\\\\24592774_Dataset_Discovery_Using_LLM\\\\MetaData_Creation\\\\MetaData_Notebooks\\\\Prepared_MetaData_DataSet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MetaData_DS = Metadata.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MetaData_DS['text'] = MetaData_DS['title']+MetaData_DS['description']+MetaData_DS['summary']+MetaData_DS['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Temp\\ipykernel_9880\\1282858165.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_ids = torch.tensor(train_ids)\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Temp\\ipykernel_9880\\1282858165.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_ids = torch.tensor(val_ids)\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Temp\\ipykernel_9880\\1282858165.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_masks = torch.tensor(train_masks)\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Temp\\ipykernel_9880\\1282858165.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_masks = torch.tensor(val_masks)\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 0.43093207478523254, Train Accuracy: 1.0\n",
      "Validation Loss: 0.3218614310026169, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 2/10\n",
      "Train Loss: 0.363043999671936, Train Accuracy: 0.9701492537313433\n",
      "Validation Loss: 0.24489563703536987, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 3/10\n",
      "Train Loss: 0.29445854425430296, Train Accuracy: 1.0\n",
      "Validation Loss: 0.1874486580491066, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 4/10\n",
      "Train Loss: 0.2625820904970169, Train Accuracy: 1.0\n",
      "Validation Loss: 0.14426851272583008, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 5/10\n",
      "Train Loss: 0.21354144513607026, Train Accuracy: 1.0\n",
      "Validation Loss: 0.11172188073396683, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 6/10\n",
      "Train Loss: 0.16885188221931458, Train Accuracy: 1.0\n",
      "Validation Loss: 0.08720003440976143, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 7/10\n",
      "Train Loss: 0.13077781200408936, Train Accuracy: 1.0\n",
      "Validation Loss: 0.0675053782761097, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 8/10\n",
      "Train Loss: 0.12535317689180375, Train Accuracy: 1.0\n",
      "Validation Loss: 0.052766142413020134, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 9/10\n",
      "Train Loss: 0.09820499122142792, Train Accuracy: 1.0\n",
      "Validation Loss: 0.04149593412876129, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 10/10\n",
      "Train Loss: 0.08781631141901017, Train Accuracy: 1.0\n",
      "Validation Loss: 0.03376421891152859, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import T5Tokenizer, T5Model, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Initialize T5 tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "\n",
    "# Define maximum sequence length\n",
    "max_length = 128  \n",
    "\n",
    "# Tokenize dataset descriptions\n",
    "tokenized_descriptions = tokenizer(list(MetaData_DS['description']), truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
    "\n",
    "# Convert labels to numerical format\n",
    "labels = [1 if url else 0 for url in MetaData_DS['dataset_url']]\n",
    "\n",
    "# Extract input_ids and attention_mask\n",
    "input_ids = tokenized_descriptions['input_ids']\n",
    "attention_mask = tokenized_descriptions['attention_mask']\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_ids, val_ids, train_masks, val_masks, train_labels, val_labels = train_test_split(input_ids, attention_mask, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert tokenized texts and labels to PyTorch tensors\n",
    "train_ids = torch.tensor(train_ids)\n",
    "val_ids = torch.tensor(val_ids)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "# Create TensorDataset for training and validation sets\n",
    "train_dataset = TensorDataset(train_ids, train_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_ids, val_masks, val_labels)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Create DataLoader for training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the T5-based model with increased dropout\n",
    "class T5Classifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(T5Classifier, self).__init__()\n",
    "        self.t5 = T5Model.from_pretrained('t5-base')\n",
    "        self.dropout = nn.Dropout(0.3)  # Dropout rate\n",
    "        self.linear = nn.Linear(self.t5.config.d_model, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.t5.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = outputs.last_hidden_state[:, 0, :]  # Take [CLS] token representation\n",
    "        hidden_state = self.dropout(hidden_state)\n",
    "        logits = self.linear(hidden_state)\n",
    "        return logits\n",
    "\n",
    "# Instantiate the T5 model\n",
    "num_classes = 2  \n",
    "model = T5Classifier(num_classes)\n",
    "\n",
    "# Define the optimizer with weight decay and gradient clipping\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=1, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop with early stopping\n",
    "epochs = 10\n",
    "patience = 2\n",
    "best_loss = float('inf')\n",
    "trigger_times = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for batch in train_loader:\n",
    "        inputs, attention_masks, labels = batch\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs, attention_masks)\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy for this batch\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = total_correct / total_samples\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, attention_masks, labels = batch\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs, attention_masks)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Compute predictions\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "            # Update total samples and total correct predictions\n",
    "            total_samples += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Save predictions and labels for metric calculation\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='binary')\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    print(f\"Train Loss: {avg_train_loss}, Train Accuracy: {train_accuracy}\")\n",
    "    print(f\"Validation Loss: {avg_val_loss}, Validation Accuracy: {accuracy}\")\n",
    "    print(f\"Validation Precision: {precision}, Validation Recall: {recall}, Validation F1 Score: {f1}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        trigger_times = 0\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print('Early stopping!')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Temp\\ipykernel_9880\\719962377.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_ids = torch.tensor(train_ids)\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Temp\\ipykernel_9880\\719962377.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_ids = torch.tensor(val_ids)\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Temp\\ipykernel_9880\\719962377.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_masks = torch.tensor(train_masks)\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Temp\\ipykernel_9880\\719962377.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_masks = torch.tensor(val_masks)\n",
      "C:\\Users\\Steffi Grace\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 0.714879310131073, Train Accuracy: 0.417910447761194\n",
      "Validation Loss: 0.5631105601787567, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 2/10\n",
      "Train Loss: 0.5656410336494446, Train Accuracy: 0.8507462686567164\n",
      "Validation Loss: 0.4415822774171829, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 3/10\n",
      "Train Loss: 0.45174430012702943, Train Accuracy: 0.9850746268656716\n",
      "Validation Loss: 0.34335102140903473, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 4/10\n",
      "Train Loss: 0.34579399824142454, Train Accuracy: 1.0\n",
      "Validation Loss: 0.25897257030010223, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 5/10\n",
      "Train Loss: 0.2665411353111267, Train Accuracy: 1.0\n",
      "Validation Loss: 0.1863223910331726, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 6/10\n",
      "Train Loss: 0.2128537952899933, Train Accuracy: 1.0\n",
      "Validation Loss: 0.12977488338947296, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 7/10\n",
      "Train Loss: 0.15388786494731904, Train Accuracy: 1.0\n",
      "Validation Loss: 0.09271147102117538, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 8/10\n",
      "Train Loss: 0.11058298647403716, Train Accuracy: 1.0\n",
      "Validation Loss: 0.06774421967566013, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 9/10\n",
      "Train Loss: 0.0928366094827652, Train Accuracy: 1.0\n",
      "Validation Loss: 0.05031730979681015, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n",
      "Epoch 10/10\n",
      "Train Loss: 0.07089263126254082, Train Accuracy: 1.0\n",
      "Validation Loss: 0.03835412301123142, Validation Accuracy: 1.0\n",
      "Validation Precision: 1.0, Validation Recall: 1.0, Validation F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import T5Tokenizer, T5Model, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Initialize T5 tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "\n",
    "# Define maximum sequence length\n",
    "max_length = 128  \n",
    "\n",
    "# Tokenize dataset descriptions\n",
    "tokenized_descriptions = tokenizer(list(MetaData_DS['text']), truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
    "\n",
    "# Convert labels to numerical format\n",
    "labels = [1 if url else 0 for url in MetaData_DS['dataset_url']]\n",
    "\n",
    "# Extract input_ids and attention_mask\n",
    "input_ids = tokenized_descriptions['input_ids']\n",
    "attention_mask = tokenized_descriptions['attention_mask']\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_ids, val_ids, train_masks, val_masks, train_labels, val_labels = train_test_split(input_ids, attention_mask, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert tokenized texts and labels to PyTorch tensors\n",
    "train_ids = torch.tensor(train_ids)\n",
    "val_ids = torch.tensor(val_ids)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "# Create TensorDataset for training and validation sets\n",
    "train_dataset = TensorDataset(train_ids, train_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_ids, val_masks, val_labels)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Create DataLoader for training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the T5-based model with increased dropout\n",
    "class T5Classifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(T5Classifier, self).__init__()\n",
    "        self.t5 = T5Model.from_pretrained('t5-base')\n",
    "        self.dropout = nn.Dropout(0.3)  # Dropout rate\n",
    "        self.linear = nn.Linear(self.t5.config.d_model, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.t5.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = outputs.last_hidden_state[:, 0, :]  # Take [CLS] token representation\n",
    "        hidden_state = self.dropout(hidden_state)\n",
    "        logits = self.linear(hidden_state)\n",
    "        return logits\n",
    "\n",
    "# Instantiate the T5 model\n",
    "num_classes = 2 \n",
    "model = T5Classifier(num_classes)\n",
    "\n",
    "# Define the optimizer with weight decay and gradient clipping\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=1, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop with early stopping\n",
    "epochs = 10\n",
    "patience = 2\n",
    "best_loss = float('inf')\n",
    "trigger_times = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for batch in train_loader:\n",
    "        inputs, attention_masks, labels = batch\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs, attention_masks)\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy for this batch\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = total_correct / total_samples\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, attention_masks, labels = batch\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs, attention_masks)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Compute predictions\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "            # Update total samples and total correct predictions\n",
    "            total_samples += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Save predictions and labels for metric calculation\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='binary')\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    print(f\"Train Loss: {avg_train_loss}, Train Accuracy: {train_accuracy}\")\n",
    "    print(f\"Validation Loss: {avg_val_loss}, Validation Accuracy: {accuracy}\")\n",
    "    print(f\"Validation Precision: {precision}, Validation Recall: {recall}, Validation F1 Score: {f1}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        trigger_times = 0\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print('Early stopping!')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ['test1234']\n",
      "Dataset URL: https://data.world/kgs1/test1234\n",
      "Cosine Similarity: 0.16620910167694092\n",
      "\n",
      "Title: ['legalmap']\n",
      "Dataset URL: https://data.world/h0tftw/legalmap\n",
      "Cosine Similarity: 0.16620910167694092\n",
      "\n",
      "Title: ['iptv', 'subscription', 'service', 'go']\n",
      "Dataset URL: https://data.world/freemotion/which-iptv-subscription-service-should-you-go-for\n",
      "Cosine Similarity: 0.16620910167694092\n",
      "\n",
      "Title: ['recognized', 'sports']\n",
      "Dataset URL: https://data.world/sports/recognized-sports\n",
      "Cosine Similarity: 0.16620910167694092\n",
      "\n",
      "Title: ['sports', 'illustrated', 'covers']\n",
      "Dataset URL: https://data.world/crowdflower/sports-illustrated-covers\n",
      "Cosine Similarity: 0.16620910167694092\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5Model\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load T5 tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "model = T5Model.from_pretrained('t5-base')\n",
    "model.eval()\n",
    "\n",
    "# Function to get T5 embedding\n",
    "def get_t5_embedding(text, model, tokenizer, max_length=128):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=max_length)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.encoder(**inputs)\n",
    "    # Use the last hidden state of the [CLS] token as the embedding\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "\n",
    "dataset_descriptions = MetaData_DS['description']\n",
    "\n",
    "# Generate embeddings for all dataset descriptions\n",
    "dataset_embeddings = []\n",
    "for description in dataset_descriptions:\n",
    "    emb = get_t5_embedding(description, model, tokenizer)\n",
    "    dataset_embeddings.append(emb)\n",
    "\n",
    "# Function to predict dataset URLs based on search text\n",
    "def predict_dataset_url(search_text):\n",
    "    search_emb = get_t5_embedding(search_text, model, tokenizer)\n",
    "    similarities = [cosine_similarity([search_emb], [dataset_emb])[0][0] for dataset_emb in dataset_embeddings]\n",
    "    \n",
    "    # Find the maximum and minimum similarity values\n",
    "    max_similarity = max(similarities)\n",
    "    min_similarity = min(similarities)\n",
    "    \n",
    "    # Calculate the 90% range\n",
    "    range_90 = 0.90 * (max_similarity - min_similarity)\n",
    "    \n",
    "    # Find the threshold value\n",
    "    threshold = min_similarity + range_90\n",
    "    \n",
    "    # Find the indices of datasets with similarity above the threshold\n",
    "    similar_indices = [i for i, sim in enumerate(similarities) if sim > threshold]\n",
    "    \n",
    "    results = []\n",
    "    for idx in similar_indices:\n",
    "        dataset_info = {\n",
    "            \"title\": MetaData_DS.iloc[idx]['title'],\n",
    "            \"dataset_url\": MetaData_DS.iloc[idx]['dataset_url'],\n",
    "            \"cosine_similarity\": similarities[idx]\n",
    "        }\n",
    "        results.append(dataset_info)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "search_text = \"electric vehicles\"\n",
    "results = predict_dataset_url(search_text)\n",
    "\n",
    "# Display results\n",
    "if not results:\n",
    "    print(\"No datasets found above the 90% threshold.\")\n",
    "else:\n",
    "    for result in results:\n",
    "        print(f\"Title: {result['title']}\")\n",
    "        print(f\"Dataset URL: {result['dataset_url']}\")\n",
    "        print(f\"Cosine Similarity: {result['cosine_similarity']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ['test1234']\n",
      "Dataset URL: https://data.world/kgs1/test1234\n",
      "Cosine Similarity: 0.13548383116722107\n",
      "\n",
      "Title: ['legalmap']\n",
      "Dataset URL: https://data.world/h0tftw/legalmap\n",
      "Cosine Similarity: 0.1409246027469635\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5Model\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load T5 tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "model = T5Model.from_pretrained('t5-base')\n",
    "model.eval()\n",
    "\n",
    "# Function to get T5 embedding\n",
    "def get_t5_embedding(text, model, tokenizer, max_length=128):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=max_length)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.encoder(**inputs)\n",
    "    # Use the last hidden state of the [CLS] token as the embedding\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "\n",
    "dataset_descriptions = MetaData_DS['text']\n",
    "\n",
    "# Generate embeddings for all dataset descriptions\n",
    "dataset_embeddings = []\n",
    "for description in dataset_descriptions:\n",
    "    emb = get_t5_embedding(description, model, tokenizer)\n",
    "    dataset_embeddings.append(emb)\n",
    "\n",
    "# Function to predict dataset URLs based on search text\n",
    "def predict_dataset_url(search_text):\n",
    "    search_emb = get_t5_embedding(search_text, model, tokenizer)\n",
    "    similarities = [cosine_similarity([search_emb], [dataset_emb])[0][0] for dataset_emb in dataset_embeddings]\n",
    "    \n",
    "    # Find the maximum and minimum similarity values\n",
    "    max_similarity = max(similarities)\n",
    "    min_similarity = min(similarities)\n",
    "    \n",
    "    # Calculate the 90% range\n",
    "    range_90 = 0.90 * (max_similarity - min_similarity)\n",
    "    \n",
    "    # Find the threshold value\n",
    "    threshold = min_similarity + range_90\n",
    "    \n",
    "    # Find the indices of datasets with similarity above the threshold\n",
    "    similar_indices = [i for i, sim in enumerate(similarities) if sim > threshold]\n",
    "    \n",
    "    results = []\n",
    "    for idx in similar_indices:\n",
    "        dataset_info = {\n",
    "            \"title\": MetaData_DS.iloc[idx]['title'],\n",
    "            \"dataset_url\": MetaData_DS.iloc[idx]['dataset_url'],\n",
    "            \"cosine_similarity\": similarities[idx]\n",
    "        }\n",
    "        results.append(dataset_info)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "search_text = \"electric vehicles\"\n",
    "results = predict_dataset_url(search_text)\n",
    "\n",
    "# Display results\n",
    "if not results:\n",
    "    print(\"No datasets found above the 90% threshold.\")\n",
    "else:\n",
    "    for result in results:\n",
    "        print(f\"Title: {result['title']}\")\n",
    "        print(f\"Dataset URL: {result['dataset_url']}\")\n",
    "        print(f\"Cosine Similarity: {result['cosine_similarity']}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
